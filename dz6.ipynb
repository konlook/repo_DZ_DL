{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1.\n",
    "\n",
    "Обучите нейронную сеть решать шифр Цезаря.\n",
    "\n",
    "Что необходимо сделать:\n",
    "\n",
    "* Написать алгоритм шифра Цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
    "* Сделать нейронную сеть\n",
    "* Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
    "* Проверить качество\n",
    "\n",
    "Задание 2.\n",
    "\n",
    "Выполнить практическую работу из лекционного ноутбука.\n",
    "\n",
    "* Построить RNN-ячейку на основе полносвязных слоев\n",
    "* Применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # для работы с данными\n",
    "import time  # для оценки времени\n",
    "import torch  # для написания нейросети\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_cifer_cesar(phrase,num_K):\n",
    "    \n",
    "    abvgd = 'abcdefghijklmnopqrstuvwxyz' \n",
    "    if num_K>len(abvgd):\n",
    "        num_K=len(abvgd)\n",
    "\n",
    "    ab_ba=abvgd+abvgd[::-1]\n",
    "\n",
    "    w_i_dict = {w: i for i, w in enumerate(abvgd)} \n",
    "\n",
    "    loc_lst=''\n",
    "    for letter in phrase:\n",
    "        if letter in abvgd:\n",
    "            loc_lst+=ab_ba[w_i_dict[letter]+num_K]\n",
    "        else:\n",
    "            loc_lst+=letter\n",
    "        \n",
    "    return loc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_clean_text(phrase):\n",
    "    \n",
    "    abvgd = 'abcdefghijklmnopqrstuvwxyz ' \n",
    "\n",
    "    w_i_dict = {w: i for i, w in enumerate(abvgd)} \n",
    "\n",
    "    loc_lst=''\n",
    "    for letter in phrase.lower():\n",
    "        if letter in abvgd:\n",
    "            loc_lst+=abvgd[w_i_dict[letter]]\n",
    "        else:\n",
    "            loc_lst+=''\n",
    "        \n",
    "    return loc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_text_to_num(phrase):\n",
    "    \n",
    "    abvgd = 'abcdefghijklmnopqrstuvwxyz ' \n",
    "\n",
    "    w_i_dict = {w: i for i, w in enumerate(abvgd)} \n",
    "\n",
    "    loc_lst=[]\n",
    "    for letter in phrase.lower():\n",
    "        if letter in abvgd:\n",
    "            loc_lst.append(w_i_dict[letter])\n",
    "        \n",
    "    return torch.Tensor(loc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_num_to_text(lst):\n",
    "    \n",
    "    abvgd = 'abcdefghijklmnopqrstuvwxyz ' \n",
    "    lst=list(lst.numpy())\n",
    "    i_w_dict = {i: w for i, w in enumerate(abvgd)} \n",
    "\n",
    "    loc_lst=''\n",
    "    for num in lst:\n",
    "        if num in range(len(abvgd)):\n",
    "            loc_lst+=i_w_dict[num]\n",
    "        \n",
    "    return loc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwerty   uiop'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_num_to_text(f_text_to_num('qwerty   uiop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_cadr_text(phrase,lenght):\n",
    "\n",
    "    loc_lst=phrase\n",
    "    for i in range(lenght-len(phrase)):\n",
    "        loc_lst+=' '\n",
    "        \n",
    "    return loc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10891 entries, 0 to 10890\n",
      "Data columns (total 1 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   normalized_text  10891 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 85.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv',usecols=['normalized_text'])\n",
    "\n",
    "df=df.dropna().reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']=[f_clean_text(x) for x in df['normalized_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392,\n",
       " '\\n',\n",
       " 'elegy for geezer rock postcard image thing to see to think of springfield is to think of thee what thoughts bepass ahind thy mien why sky art blue why trees art green and what pray tell did thine eyes see perchance old friend they gazed at me brought low by natures oafish hand thou crushed our reviewing stand and twixt thy stones glimpsed i the truth all things must pass  thy face my youth')"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_max=[len(x) for x in df['clean_text']]\n",
    "\n",
    "lenght_max=lst_max[np.argmax(lst_max)]\n",
    "\n",
    "lenght_max,'\\n',df['clean_text'][np.argmax(lst_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_cadr_text(df['clean_text'][0],lenght_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cadr_text']=[f_cadr_text(x,lenght_max) for x in df['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_cifer=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>cadr_text</th>\n",
       "      <th>cifer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maggie look whats that</td>\n",
       "      <td>maggie look whats that</td>\n",
       "      <td>maggie look whats that                        ...</td>\n",
       "      <td>ozuusw pmmq etzhi htzh                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lee-mur lee-mur</td>\n",
       "      <td>leemur leemur</td>\n",
       "      <td>leemur leemur                                 ...</td>\n",
       "      <td>pwwogj pwwogj                                 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zee-boo zee-boo</td>\n",
       "      <td>zeeboo zeeboo</td>\n",
       "      <td>zeeboo zeeboo                                 ...</td>\n",
       "      <td>bwwzmm bwwzmm                                 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>so hjcsnu hm hwzyt ozuusw htzh nzhgjw xmwinh w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>shi psqw zn md mnpc sh tzi z tgol znx z xwepzl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     normalized_text  \\\n",
       "0                             maggie look whats that   \n",
       "1                                    lee-mur lee-mur   \n",
       "2                                    zee-boo zee-boo   \n",
       "3  im trying to teach maggie that nature doesnt e...   \n",
       "4  its like an ox only it has a hump and a dewlap...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                             maggie look whats that   \n",
       "1                                      leemur leemur   \n",
       "2                                      zeeboo zeeboo   \n",
       "3  im trying to teach maggie that nature doesnt e...   \n",
       "4  its like an ox only it has a hump and a dewlap...   \n",
       "\n",
       "                                           cadr_text  \\\n",
       "0  maggie look whats that                        ...   \n",
       "1  leemur leemur                                 ...   \n",
       "2  zeeboo zeeboo                                 ...   \n",
       "3  im trying to teach maggie that nature doesnt e...   \n",
       "4  its like an ox only it has a hump and a dewlap...   \n",
       "\n",
       "                                          cifer_text  \n",
       "0  ozuusw pmmq etzhi htzh                        ...  \n",
       "1  pwwogj pwwogj                                 ...  \n",
       "2  bwwzmm bwwzmm                                 ...  \n",
       "3  so hjcsnu hm hwzyt ozuusw htzh nzhgjw xmwinh w...  \n",
       "4  shi psqw zn md mnpc sh tzi z tgol znx z xwepzl...  "
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cifer_text']=[f_cifer_cesar(x,K_cifer) for x in df['cadr_text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cadr_text</th>\n",
       "      <th>cifer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maggie look whats that                        ...</td>\n",
       "      <td>ozuusw pmmq etzhi htzh                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leemur leemur                                 ...</td>\n",
       "      <td>pwwogj pwwogj                                 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zeeboo zeeboo                                 ...</td>\n",
       "      <td>bwwzmm bwwzmm                                 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>so hjcsnu hm hwzyt ozuusw htzh nzhgjw xmwinh w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>shi psqw zn md mnpc sh tzi z tgol znx z xwepzl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cadr_text  \\\n",
       "0  maggie look whats that                        ...   \n",
       "1  leemur leemur                                 ...   \n",
       "2  zeeboo zeeboo                                 ...   \n",
       "3  im trying to teach maggie that nature doesnt e...   \n",
       "4  its like an ox only it has a hump and a dewlap...   \n",
       "\n",
       "                                          cifer_text  \n",
       "0  ozuusw pmmq etzhi htzh                        ...  \n",
       "1  pwwogj pwwogj                                 ...  \n",
       "2  bwwzmm bwwzmm                                 ...  \n",
       "3  so hjcsnu hm hwzyt ozuusw htzh nzhgjw xmwinh w...  \n",
       "4  shi psqw zn md mnpc sh tzi z tgol znx z xwepzl...  "
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['cadr_text','cifer_text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['cadr_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cifer_vec']=[f_text_to_num(x) for x in df['cifer_text']]\n",
    "df['cadr_vec']=[f_text_to_num(x) for x in df['cadr_text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['cifer_vec'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cadr_text</th>\n",
       "      <th>cifer_text</th>\n",
       "      <th>cifer_vec</th>\n",
       "      <th>cadr_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maggie look whats that                        ...</td>\n",
       "      <td>ozuusw pmmq etzhi htzh                        ...</td>\n",
       "      <td>[tensor(14.), tensor(25.), tensor(20.), tensor...</td>\n",
       "      <td>[tensor(12.), tensor(0.), tensor(6.), tensor(6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leemur leemur                                 ...</td>\n",
       "      <td>pwwogj pwwogj                                 ...</td>\n",
       "      <td>[tensor(15.), tensor(22.), tensor(22.), tensor...</td>\n",
       "      <td>[tensor(11.), tensor(4.), tensor(4.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zeeboo zeeboo                                 ...</td>\n",
       "      <td>bwwzmm bwwzmm                                 ...</td>\n",
       "      <td>[tensor(1.), tensor(22.), tensor(22.), tensor(...</td>\n",
       "      <td>[tensor(25.), tensor(4.), tensor(4.), tensor(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
       "      <td>so hjcsnu hm hwzyt ozuusw htzh nzhgjw xmwinh w...</td>\n",
       "      <td>[tensor(18.), tensor(14.), tensor(26.), tensor...</td>\n",
       "      <td>[tensor(8.), tensor(12.), tensor(26.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
       "      <td>shi psqw zn md mnpc sh tzi z tgol znx z xwepzl...</td>\n",
       "      <td>[tensor(18.), tensor(7.), tensor(8.), tensor(2...</td>\n",
       "      <td>[tensor(8.), tensor(19.), tensor(18.), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cadr_text  \\\n",
       "0  maggie look whats that                        ...   \n",
       "1  leemur leemur                                 ...   \n",
       "2  zeeboo zeeboo                                 ...   \n",
       "3  im trying to teach maggie that nature doesnt e...   \n",
       "4  its like an ox only it has a hump and a dewlap...   \n",
       "\n",
       "                                          cifer_text  \\\n",
       "0  ozuusw pmmq etzhi htzh                        ...   \n",
       "1  pwwogj pwwogj                                 ...   \n",
       "2  bwwzmm bwwzmm                                 ...   \n",
       "3  so hjcsnu hm hwzyt ozuusw htzh nzhgjw xmwinh w...   \n",
       "4  shi psqw zn md mnpc sh tzi z tgol znx z xwepzl...   \n",
       "\n",
       "                                           cifer_vec  \\\n",
       "0  [tensor(14.), tensor(25.), tensor(20.), tensor...   \n",
       "1  [tensor(15.), tensor(22.), tensor(22.), tensor...   \n",
       "2  [tensor(1.), tensor(22.), tensor(22.), tensor(...   \n",
       "3  [tensor(18.), tensor(14.), tensor(26.), tensor...   \n",
       "4  [tensor(18.), tensor(7.), tensor(8.), tensor(2...   \n",
       "\n",
       "                                            cadr_vec  \n",
       "0  [tensor(12.), tensor(0.), tensor(6.), tensor(6...  \n",
       "1  [tensor(11.), tensor(4.), tensor(4.), tensor(1...  \n",
       "2  [tensor(25.), tensor(4.), tensor(4.), tensor(1...  \n",
       "3  [tensor(8.), tensor(12.), tensor(26.), tensor(...  \n",
       "4  [tensor(8.), tensor(19.), tensor(18.), tensor(...  "
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cifer_vec'], df['cadr_vec'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reset_index(drop=True)\n",
    "X_test=X_test.reset_index(drop=True)\n",
    "y_train=y_train.reset_index(drop=True)\n",
    "y_test=y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.Series([torch.Tensor(x).reshape(1,1,392) for x in X_train])\n",
    "X_test= pd.Series([torch.Tensor(x).reshape(1,1,392) for x in  X_test])\n",
    "y_train=pd.Series([torch.Tensor(x).reshape(1,1,392) for x in y_train])\n",
    "y_test= pd.Series([torch.Tensor(x).reshape(1,1,392) for x in  y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[[tensor(4.), tensor(22.), tensor(15.), tenso...\n",
       "1    [[[tensor(13.), tensor(12.), tensor(26.), tens...\n",
       "2    [[[tensor(7.), tensor(19.), tensor(25.), tenso...\n",
       "3    [[[tensor(25.), tensor(13.), tensor(2.), tenso...\n",
       "4    [[[tensor(7.), tensor(19.), tensor(25.), tenso...\n",
       "dtype: object"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2179"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5, 4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        locX_tens=features[i]\n",
    "        locY_tens=labels[i]\n",
    "\n",
    "        if (i+batch_size)>num_examples:\n",
    "            continue\n",
    "        \n",
    "        j = indices[i+1: i + batch_size]#min(i + batch_size, num_examples)]\n",
    "        for k in j:\n",
    "            locX_tens=torch.cat((locX_tens,features[k]))\n",
    "            locY_tens=torch.cat((locY_tens,features[k]))\n",
    "\n",
    "        yield locX_tens,locY_tens #features[j], labels[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 392])"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.Tensor(1,1,392)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "for X, y in data_iter(batch_size, X_train, y_train):\n",
    "    print(X.shape,'\\n', y.shape)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = torch.nn.Conv2d(in_channels=1,\n",
    "                         out_channels=1,\n",
    "                         kernel_size=(1,1))'''\n",
    "\n",
    "'''model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(392, 2048),\n",
    "    torch.nn.BatchNorm1d(2048),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(2048, 1024),\n",
    "    torch.nn.BatchNorm1d(1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(1024, 512),\n",
    "    torch.nn.BatchNorm1d(512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(512, 392)\n",
    ")'''\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(392, 512),\n",
    "    \n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(512, 392))#torch.nn.Flatten(),torch.nn.BatchNorm1d(512),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''optimizer = torch.optim.SGD(conv2d.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()'''\n",
    "loss = torch.nn.MSELoss(reduction='sum')\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 392]) torch.Size([1, 1, 392])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter(1, X_train, y_train):\n",
    "            trainer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            print(X.shape,y.shape)\n",
    "            break\n",
    "            l = loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n",
    "\n",
    "input.shape'''\n",
    "num_epochs = 10\n",
    "batch_size=1\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        train_iters, train_passed  = 0, 0\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        start=time.time()\n",
    "        \n",
    "        model.train()\n",
    "        for X, y in data_iter(batch_size, X_train, y_train):\n",
    "            trainer.zero_grad()\n",
    "            y_pred = model(X).squeeze()\n",
    "            l = loss(y_pred, y.squeeze())\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_loss += l.item()\n",
    "            train_acc += (y_pred == y.squeeze()).sum().item()#(y_pred.argmax(dim=1) == y).sum().item()\n",
    "            train_iters += 1\n",
    "            train_passed += len(X)\n",
    "        \n",
    "        test_iters, test_passed  = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "        model.eval()\n",
    "        for X, y in data_iter(batch_size, X_test, y_test):\n",
    "            y_pred = model(X).squeeze()\n",
    "            l = loss(y_pred, y.squeeze())\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred == y.squeeze()).sum().item()#(y_pred.argmax(dim=1) == y).sum().item()\n",
    "            test_iters += 1\n",
    "            test_passed += len(X)\n",
    "            \n",
    "        print(\"epoch: {}, taked_time: {:.3f}, train_loss: {:.3f}, train_acc: {:.3f}, test_loss: {:.3f}, test_acc: {:.3f}\".format(\n",
    "            ep, time.time() - start, train_loss / train_iters, train_acc / train_passed,\n",
    "            test_loss / test_iters, test_acc / test_passed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, taked_time: 80.471, train_loss: 6356.902, train_acc: 28.740, test_loss: 6281.906, test_acc: 4.995\n",
      "epoch: 1, taked_time: 116.150, train_loss: 6340.898, train_acc: 5.619, test_loss: 6279.336, test_acc: 4.995\n",
      "epoch: 2, taked_time: 102.616, train_loss: 6339.362, train_acc: 5.619, test_loss: 6279.356, test_acc: 4.995\n",
      "epoch: 3, taked_time: 104.316, train_loss: 6339.237, train_acc: 5.619, test_loss: 6279.466, test_acc: 4.995\n",
      "epoch: 4, taked_time: 103.901, train_loss: 6339.246, train_acc: 5.619, test_loss: 6279.515, test_acc: 4.995\n",
      "epoch: 5, taked_time: 108.515, train_loss: 6339.257, train_acc: 5.619, test_loss: 6279.532, test_acc: 4.995\n",
      "epoch: 6, taked_time: 117.579, train_loss: 6339.262, train_acc: 5.619, test_loss: 6279.538, test_acc: 4.995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-569-fd66dc043b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-566-b05ba74786e1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#(y_pred.argmax(dim=1) == y).sum().item()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Users\\Konst\\anaconda3\\envs\\my_3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Users\\Konst\\anaconda3\\envs\\my_3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Users\\Konst\\anaconda3\\envs\\my_3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 state_steps)\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             adam(\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Users\\Konst\\anaconda3\\envs\\my_3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Users\\Konst\\anaconda3\\envs\\my_3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size=1\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maggie look whats that', 'lee-mur lee-mur', 'zee-boo zee-boo']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = df['normalized_text'].tolist()  # колонка с предобработанными текстами\n",
    "phrases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[c for c in ph] for ph in phrases[:3] if type(ph) is str]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')  # все символы, которые мы хотим использовать для кодировки = наш словарь\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]  # все неизвестные символы будут получать тег none\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}  # словарь токен-индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50  # мы хотим ограничить максимальную длину ввода\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов\n",
    "for i in range(len(text)):  # для каждого предложения\n",
    "    for j, w in enumerate(text[i]):  # для каждого токена\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
