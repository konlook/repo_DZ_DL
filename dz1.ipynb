{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston Housing Dataset\n",
    "\n",
    "\n",
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n",
    "\n",
    "\n",
    "CRIM - per capita crime rate by town\n",
    "\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS - proportion of non-retail business acres per town.\n",
    "\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM - average number of rooms per dwelling \n",
    "\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "RAD - index of accessibility to radial highways\n",
    "\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "LSTAT - % lower status of the population\n",
    "\n",
    "MEDV - Median value of owner-occupied homes in $1000's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-467-fe2d12490cb7>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df=pd.read_csv('BostonHousPrice\\housing.csv',sep=r'[ ]+', names=column_names)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "                 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df=pd.read_csv('BostonHousPrice\\housing.csv',sep=r'[ ]+', names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    int64  \n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    int64  \n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 59.3 KB\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: MEDV, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=pd.Series(df['MEDV'])\n",
    "del(df['MEDV'])\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "arr_df=np.array(df)\n",
    "arr_y=np.array(Y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(arr_df, arr_y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.double)\n",
    "X_test = torch.tensor(X_test, dtype=torch.double)\n",
    "y_train = torch.tensor(y_train, dtype=torch.double)\n",
    "y_test = torch.tensor(y_test, dtype=torch.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train),\n",
    "      type(X_test),\n",
    "      type(y_train),\n",
    "      type(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=torch.Size([379, 13])\n",
      "X_test.shape= torch.Size([127, 13])\n",
      "y_train.shape=torch.Size([379])\n",
      "y_test.shape= torch.Size([127])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'X_train.shape={X_train.shape}\\n'\n",
    "      f'X_test.shape= {X_test.shape}\\n'\n",
    "      f'y_train.shape={y_train.shape}\\n'\n",
    "      f'y_test.shape= {y_test.shape}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    #random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = indices[i: min(i + batch_size, num_examples)]\n",
    "        yield features[j, :], labels[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5270e-02, 0.0000e+00, 1.1930e+01, 0.0000e+00, 5.7300e-01, 6.1200e+00,\n",
      "         7.6700e+01, 2.2875e+00, 1.0000e+00, 2.7300e+02, 2.1000e+01, 3.9690e+02,\n",
      "         9.0800e+00],\n",
      "        [1.3914e-01, 0.0000e+00, 4.0500e+00, 0.0000e+00, 5.1000e-01, 5.5720e+00,\n",
      "         8.8500e+01, 2.5961e+00, 5.0000e+00, 2.9600e+02, 1.6600e+01, 3.9690e+02,\n",
      "         1.4690e+01],\n",
      "        [4.1130e-02, 2.5000e+01, 4.8600e+00, 0.0000e+00, 4.2600e-01, 6.7270e+00,\n",
      "         3.3500e+01, 5.4007e+00, 4.0000e+00, 2.8100e+02, 1.9000e+01, 3.9690e+02,\n",
      "         5.2900e+00],\n",
      "        [1.8836e-01, 0.0000e+00, 6.9100e+00, 0.0000e+00, 4.4800e-01, 5.7860e+00,\n",
      "         3.3300e+01, 5.1004e+00, 3.0000e+00, 2.3300e+02, 1.7900e+01, 3.9690e+02,\n",
      "         1.4150e+01],\n",
      "        [4.0202e-01, 0.0000e+00, 9.9000e+00, 0.0000e+00, 5.4400e-01, 6.3820e+00,\n",
      "         6.7200e+01, 3.5325e+00, 4.0000e+00, 3.0400e+02, 1.8400e+01, 3.9521e+02,\n",
      "         1.0360e+01],\n",
      "        [2.8750e-02, 2.8000e+01, 1.5040e+01, 0.0000e+00, 4.6400e-01, 6.2110e+00,\n",
      "         2.8900e+01, 3.6659e+00, 4.0000e+00, 2.7000e+02, 1.8200e+01, 3.9633e+02,\n",
      "         6.2100e+00],\n",
      "        [1.1578e+01, 0.0000e+00, 1.8100e+01, 0.0000e+00, 7.0000e-01, 5.0360e+00,\n",
      "         9.7000e+01, 1.7700e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n",
      "         2.5680e+01],\n",
      "        [4.4620e-02, 2.5000e+01, 4.8600e+00, 0.0000e+00, 4.2600e-01, 6.6190e+00,\n",
      "         7.0400e+01, 5.4007e+00, 4.0000e+00, 2.8100e+02, 1.9000e+01, 3.9563e+02,\n",
      "         7.2200e+00],\n",
      "        [5.5150e-02, 3.3000e+01, 2.1800e+00, 0.0000e+00, 4.7200e-01, 7.2360e+00,\n",
      "         4.1100e+01, 4.0220e+00, 7.0000e+00, 2.2200e+02, 1.8400e+01, 3.9368e+02,\n",
      "         6.9300e+00],\n",
      "        [3.3211e+00, 0.0000e+00, 1.9580e+01, 1.0000e+00, 8.7100e-01, 5.4030e+00,\n",
      "         1.0000e+02, 1.3216e+00, 5.0000e+00, 4.0300e+02, 1.4700e+01, 3.9690e+02,\n",
      "         2.6820e+01]], dtype=torch.float64) \n",
      " tensor([20.6000, 23.1000, 28.0000, 20.0000, 23.1000, 25.0000,  9.7000, 23.9000,\n",
      "        36.1000, 13.4000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, X_train, y_train):\n",
    "    print(X,'\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn((len(X_train[0])))\n",
    "b = torch.zeros((1,))\n",
    "w.requires_grad_()\n",
    "b.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w1, b1):\n",
    "  return torch.mv(X.float(),w1.float())+b1.float()\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "  return ((y_hat-y.reshape(y_hat.shape)) ** 2).mean()\n",
    "\n",
    "def sgd(params, lr):\n",
    "  for param in params:\n",
    "    param.data[:] = param - lr*param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1940, -1.1125, -0.7683, -1.2179, -0.9495, -0.0887,  2.2234, -0.3470,\n",
       "          1.1305,  0.4547, -0.3772, -0.1435,  0.4130], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-6  \n",
    "num_epochs = 1000\n",
    "batch_size=3\n",
    "\n",
    "w = torch.randn((len(X_train[0])))\n",
    "b = torch.zeros((1,))\n",
    "w.requires_grad_()\n",
    "b.requires_grad_()\n",
    "\n",
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 3346.071590\n",
      "epoch 2, loss 2215.242406\n",
      "epoch 3, loss 1479.702873\n",
      "epoch 4, loss 1001.442875\n",
      "epoch 5, loss 690.520052\n",
      "epoch 6, loss 488.430294\n",
      "epoch 7, loss 357.118781\n",
      "epoch 8, loss 271.832332\n",
      "epoch 9, loss 216.469874\n",
      "epoch 10, loss 180.557896\n",
      "epoch 11, loss 157.283971\n",
      "epoch 12, loss 142.217125\n",
      "epoch 13, loss 132.475788\n",
      "epoch 14, loss 126.185918\n",
      "epoch 15, loss 122.129208\n",
      "epoch 16, loss 119.513857\n",
      "epoch 17, loss 117.825303\n",
      "epoch 18, loss 116.729669\n",
      "epoch 19, loss 116.010354\n",
      "epoch 20, loss 115.526924\n",
      "epoch 21, loss 115.188741\n",
      "epoch 22, loss 114.937359\n",
      "epoch 23, loss 114.735279\n",
      "epoch 24, loss 114.558550\n",
      "epoch 25, loss 114.392100\n",
      "epoch 26, loss 114.226615\n",
      "epoch 27, loss 114.056324\n",
      "epoch 28, loss 113.878026\n",
      "epoch 29, loss 113.689876\n",
      "epoch 30, loss 113.491068\n",
      "epoch 31, loss 113.281485\n",
      "epoch 32, loss 113.061279\n",
      "epoch 33, loss 112.830923\n",
      "epoch 34, loss 112.590987\n",
      "epoch 35, loss 112.341910\n",
      "epoch 36, loss 112.084540\n",
      "epoch 37, loss 111.819482\n",
      "epoch 38, loss 111.547382\n",
      "epoch 39, loss 111.268933\n",
      "epoch 40, loss 110.984646\n",
      "epoch 41, loss 110.695148\n",
      "epoch 42, loss 110.400995\n",
      "epoch 43, loss 110.102617\n",
      "epoch 44, loss 109.800585\n",
      "epoch 45, loss 109.495299\n",
      "epoch 46, loss 109.187198\n",
      "epoch 47, loss 108.876655\n",
      "epoch 48, loss 108.564038\n",
      "epoch 49, loss 108.249674\n",
      "epoch 50, loss 107.933888\n",
      "epoch 51, loss 107.617003\n",
      "epoch 52, loss 107.299277\n",
      "epoch 53, loss 106.980901\n",
      "epoch 54, loss 106.662130\n",
      "epoch 55, loss 106.343247\n",
      "epoch 56, loss 106.024334\n",
      "epoch 57, loss 105.705663\n",
      "epoch 58, loss 105.387342\n",
      "epoch 59, loss 105.069575\n",
      "epoch 60, loss 104.752441\n",
      "epoch 61, loss 104.436056\n",
      "epoch 62, loss 104.120618\n",
      "epoch 63, loss 103.806194\n",
      "epoch 64, loss 103.492837\n",
      "epoch 65, loss 103.180672\n",
      "epoch 66, loss 102.869783\n",
      "epoch 67, loss 102.560209\n",
      "epoch 68, loss 102.252078\n",
      "epoch 69, loss 101.945343\n",
      "epoch 70, loss 101.640144\n",
      "epoch 71, loss 101.336510\n",
      "epoch 72, loss 101.034462\n",
      "epoch 73, loss 100.734047\n",
      "epoch 74, loss 100.435329\n",
      "epoch 75, loss 100.138269\n",
      "epoch 76, loss 99.842954\n",
      "epoch 77, loss 99.549347\n",
      "epoch 78, loss 99.257507\n",
      "epoch 79, loss 98.967437\n",
      "epoch 80, loss 98.679124\n",
      "epoch 81, loss 98.392603\n",
      "epoch 82, loss 98.107908\n",
      "epoch 83, loss 97.825017\n",
      "epoch 84, loss 97.543939\n",
      "epoch 85, loss 97.264652\n",
      "epoch 86, loss 96.987205\n",
      "epoch 87, loss 96.711564\n",
      "epoch 88, loss 96.437672\n",
      "epoch 89, loss 96.165662\n",
      "epoch 90, loss 95.895418\n",
      "epoch 91, loss 95.626970\n",
      "epoch 92, loss 95.360321\n",
      "epoch 93, loss 95.095467\n",
      "epoch 94, loss 94.832360\n",
      "epoch 95, loss 94.571083\n",
      "epoch 96, loss 94.311532\n",
      "epoch 97, loss 94.053715\n",
      "epoch 98, loss 93.797657\n",
      "epoch 99, loss 93.543360\n",
      "epoch 100, loss 93.290786\n",
      "epoch 101, loss 93.039886\n",
      "epoch 102, loss 92.790726\n",
      "epoch 103, loss 92.543233\n",
      "epoch 104, loss 92.297424\n",
      "epoch 105, loss 92.053272\n",
      "epoch 106, loss 91.810824\n",
      "epoch 107, loss 91.569994\n",
      "epoch 108, loss 91.330843\n",
      "epoch 109, loss 91.093281\n",
      "epoch 110, loss 90.857359\n",
      "epoch 111, loss 90.623034\n",
      "epoch 112, loss 90.390289\n",
      "epoch 113, loss 90.159140\n",
      "epoch 114, loss 89.929528\n",
      "epoch 115, loss 89.701458\n",
      "epoch 116, loss 89.474982\n",
      "epoch 117, loss 89.249988\n",
      "epoch 118, loss 89.026532\n",
      "epoch 119, loss 88.804615\n",
      "epoch 120, loss 88.584147\n",
      "epoch 121, loss 88.365173\n",
      "epoch 122, loss 88.147717\n",
      "epoch 123, loss 87.931673\n",
      "epoch 124, loss 87.717126\n",
      "epoch 125, loss 87.503987\n",
      "epoch 126, loss 87.292310\n",
      "epoch 127, loss 87.082016\n",
      "epoch 128, loss 86.873134\n",
      "epoch 129, loss 86.665645\n",
      "epoch 130, loss 86.459572\n",
      "epoch 131, loss 86.254845\n",
      "epoch 132, loss 86.051474\n",
      "epoch 133, loss 85.849447\n",
      "epoch 134, loss 85.648777\n",
      "epoch 135, loss 85.449418\n",
      "epoch 136, loss 85.251379\n",
      "epoch 137, loss 85.054677\n",
      "epoch 138, loss 84.859282\n",
      "epoch 139, loss 84.665132\n",
      "epoch 140, loss 84.472324\n",
      "epoch 141, loss 84.280766\n",
      "epoch 142, loss 84.090488\n",
      "epoch 143, loss 83.901428\n",
      "epoch 144, loss 83.713616\n",
      "epoch 145, loss 83.527062\n",
      "epoch 146, loss 83.341705\n",
      "epoch 147, loss 83.157578\n",
      "epoch 148, loss 82.974625\n",
      "epoch 149, loss 82.792868\n",
      "epoch 150, loss 82.612326\n",
      "epoch 151, loss 82.432939\n",
      "epoch 152, loss 82.254718\n",
      "epoch 153, loss 82.077657\n",
      "epoch 154, loss 81.901760\n",
      "epoch 155, loss 81.726984\n",
      "epoch 156, loss 81.553317\n",
      "epoch 157, loss 81.380820\n",
      "epoch 158, loss 81.209450\n",
      "epoch 159, loss 81.039154\n",
      "epoch 160, loss 80.869970\n",
      "epoch 161, loss 80.701863\n",
      "epoch 162, loss 80.534846\n",
      "epoch 163, loss 80.368899\n",
      "epoch 164, loss 80.204029\n",
      "epoch 165, loss 80.040226\n",
      "epoch 166, loss 79.877454\n",
      "epoch 167, loss 79.715724\n",
      "epoch 168, loss 79.555019\n",
      "epoch 169, loss 79.395342\n",
      "epoch 170, loss 79.236728\n",
      "epoch 171, loss 79.079112\n",
      "epoch 172, loss 78.922485\n",
      "epoch 173, loss 78.766859\n",
      "epoch 174, loss 78.612245\n",
      "epoch 175, loss 78.458574\n",
      "epoch 176, loss 78.305949\n",
      "epoch 177, loss 78.154217\n",
      "epoch 178, loss 78.003493\n",
      "epoch 179, loss 77.853700\n",
      "epoch 180, loss 77.704874\n",
      "epoch 181, loss 77.556981\n",
      "epoch 182, loss 77.410008\n",
      "epoch 183, loss 77.263991\n",
      "epoch 184, loss 77.118868\n",
      "epoch 185, loss 76.974676\n",
      "epoch 186, loss 76.831379\n",
      "epoch 187, loss 76.688987\n",
      "epoch 188, loss 76.547501\n",
      "epoch 189, loss 76.406905\n",
      "epoch 190, loss 76.267189\n",
      "epoch 191, loss 76.128361\n",
      "epoch 192, loss 75.990396\n",
      "epoch 193, loss 75.853267\n",
      "epoch 194, loss 75.717021\n",
      "epoch 195, loss 75.581615\n",
      "epoch 196, loss 75.447034\n",
      "epoch 197, loss 75.313297\n",
      "epoch 198, loss 75.180421\n",
      "epoch 199, loss 75.048370\n",
      "epoch 200, loss 74.917126\n",
      "epoch 201, loss 74.786715\n",
      "epoch 202, loss 74.657092\n",
      "epoch 203, loss 74.528289\n",
      "epoch 204, loss 74.400286\n",
      "epoch 205, loss 74.273062\n",
      "epoch 206, loss 74.146635\n",
      "epoch 207, loss 74.021006\n",
      "epoch 208, loss 73.896134\n",
      "epoch 209, loss 73.772016\n",
      "epoch 210, loss 73.648690\n",
      "epoch 211, loss 73.526129\n",
      "epoch 212, loss 73.404321\n",
      "epoch 213, loss 73.283260\n",
      "epoch 214, loss 73.162964\n",
      "epoch 215, loss 73.043373\n",
      "epoch 216, loss 72.924531\n",
      "epoch 217, loss 72.806425\n",
      "epoch 218, loss 72.689021\n",
      "epoch 219, loss 72.572375\n",
      "epoch 220, loss 72.456388\n",
      "epoch 221, loss 72.341159\n",
      "epoch 222, loss 72.226616\n",
      "epoch 223, loss 72.112770\n",
      "epoch 224, loss 71.999623\n",
      "epoch 225, loss 71.887160\n",
      "epoch 226, loss 71.775400\n",
      "epoch 227, loss 71.664290\n",
      "epoch 228, loss 71.553872\n",
      "epoch 229, loss 71.444118\n",
      "epoch 230, loss 71.335045\n",
      "epoch 231, loss 71.226652\n",
      "epoch 232, loss 71.118887\n",
      "epoch 233, loss 71.011783\n",
      "epoch 234, loss 70.905360\n",
      "epoch 235, loss 70.799538\n",
      "epoch 236, loss 70.694360\n",
      "epoch 237, loss 70.589815\n",
      "epoch 238, loss 70.485895\n",
      "epoch 239, loss 70.382628\n",
      "epoch 240, loss 70.279977\n",
      "epoch 241, loss 70.177946\n",
      "epoch 242, loss 70.076514\n",
      "epoch 243, loss 69.975708\n",
      "epoch 244, loss 69.875500\n",
      "epoch 245, loss 69.775888\n",
      "epoch 246, loss 69.676877\n",
      "epoch 247, loss 69.578469\n",
      "epoch 248, loss 69.480661\n",
      "epoch 249, loss 69.383434\n",
      "epoch 250, loss 69.286769\n",
      "epoch 251, loss 69.190681\n",
      "epoch 252, loss 69.095178\n",
      "epoch 253, loss 69.000254\n",
      "epoch 254, loss 68.905878\n",
      "epoch 255, loss 68.812076\n",
      "epoch 256, loss 68.718831\n",
      "epoch 257, loss 68.626130\n",
      "epoch 258, loss 68.533978\n",
      "epoch 259, loss 68.442389\n",
      "epoch 260, loss 68.351340\n",
      "epoch 261, loss 68.260817\n",
      "epoch 262, loss 68.170868\n",
      "epoch 263, loss 68.081419\n",
      "epoch 264, loss 67.992511\n",
      "epoch 265, loss 67.904129\n",
      "epoch 266, loss 67.816242\n",
      "epoch 267, loss 67.728898\n",
      "epoch 268, loss 67.642064\n",
      "epoch 269, loss 67.555752\n",
      "epoch 270, loss 67.469954\n",
      "epoch 271, loss 67.384643\n",
      "epoch 272, loss 67.299823\n",
      "epoch 273, loss 67.215517\n",
      "epoch 274, loss 67.131722\n",
      "epoch 275, loss 67.048392\n",
      "epoch 276, loss 66.965579\n",
      "epoch 277, loss 66.883220\n",
      "epoch 278, loss 66.801355\n",
      "epoch 279, loss 66.719957\n",
      "epoch 280, loss 66.639040\n",
      "epoch 281, loss 66.558598\n",
      "epoch 282, loss 66.478651\n",
      "epoch 283, loss 66.399133\n",
      "epoch 284, loss 66.320076\n",
      "epoch 285, loss 66.241522\n",
      "epoch 286, loss 66.163393\n",
      "epoch 287, loss 66.085730\n",
      "epoch 288, loss 66.008493\n",
      "epoch 289, loss 65.931727\n",
      "epoch 290, loss 65.855418\n",
      "epoch 291, loss 65.779532\n",
      "epoch 292, loss 65.704088\n",
      "epoch 293, loss 65.629082\n",
      "epoch 294, loss 65.554482\n",
      "epoch 295, loss 65.480344\n",
      "epoch 296, loss 65.406641\n",
      "epoch 297, loss 65.333344\n",
      "epoch 298, loss 65.260466\n",
      "epoch 299, loss 65.188032\n",
      "epoch 300, loss 65.115990\n",
      "epoch 301, loss 65.044366\n",
      "epoch 302, loss 64.973153\n",
      "epoch 303, loss 64.902357\n",
      "epoch 304, loss 64.831968\n",
      "epoch 305, loss 64.761969\n",
      "epoch 306, loss 64.692382\n",
      "epoch 307, loss 64.623192\n",
      "epoch 308, loss 64.554389\n",
      "epoch 309, loss 64.485983\n",
      "epoch 310, loss 64.417966\n",
      "epoch 311, loss 64.350332\n",
      "epoch 312, loss 64.283075\n",
      "epoch 313, loss 64.216216\n",
      "epoch 314, loss 64.149731\n",
      "epoch 315, loss 64.083624\n",
      "epoch 316, loss 64.017910\n",
      "epoch 317, loss 63.952544\n",
      "epoch 318, loss 63.887555\n",
      "epoch 319, loss 63.822935\n",
      "epoch 320, loss 63.758688\n",
      "epoch 321, loss 63.694785\n",
      "epoch 322, loss 63.631236\n",
      "epoch 323, loss 63.568070\n",
      "epoch 324, loss 63.505234\n",
      "epoch 325, loss 63.442783\n",
      "epoch 326, loss 63.380663\n",
      "epoch 327, loss 63.318898\n",
      "epoch 328, loss 63.257485\n",
      "epoch 329, loss 63.196401\n",
      "epoch 330, loss 63.135672\n",
      "epoch 331, loss 63.075287\n",
      "epoch 332, loss 63.015233\n",
      "epoch 333, loss 62.955517\n",
      "epoch 334, loss 62.896126\n",
      "epoch 335, loss 62.837067\n",
      "epoch 336, loss 62.778343\n",
      "epoch 337, loss 62.719952\n",
      "epoch 338, loss 62.661870\n",
      "epoch 339, loss 62.604099\n",
      "epoch 340, loss 62.546664\n",
      "epoch 341, loss 62.489553\n",
      "epoch 342, loss 62.432756\n",
      "epoch 343, loss 62.376290\n",
      "epoch 344, loss 62.320118\n",
      "epoch 345, loss 62.264248\n",
      "epoch 346, loss 62.208705\n",
      "epoch 347, loss 62.153457\n",
      "epoch 348, loss 62.098521\n",
      "epoch 349, loss 62.043874\n",
      "epoch 350, loss 61.989543\n",
      "epoch 351, loss 61.935502\n",
      "epoch 352, loss 61.881755\n",
      "epoch 353, loss 61.828303\n",
      "epoch 354, loss 61.775153\n",
      "epoch 355, loss 61.722277\n",
      "epoch 356, loss 61.669716\n",
      "epoch 357, loss 61.617440\n",
      "epoch 358, loss 61.565442\n",
      "epoch 359, loss 61.513710\n",
      "epoch 360, loss 61.462291\n",
      "epoch 361, loss 61.411126\n",
      "epoch 362, loss 61.360263\n",
      "epoch 363, loss 61.309657\n",
      "epoch 364, loss 61.259323\n",
      "epoch 365, loss 61.209267\n",
      "epoch 366, loss 61.159481\n",
      "epoch 367, loss 61.109983\n",
      "epoch 368, loss 61.060735\n",
      "epoch 369, loss 61.011752\n",
      "epoch 370, loss 60.963046\n",
      "epoch 371, loss 60.914617\n",
      "epoch 372, loss 60.866408\n",
      "epoch 373, loss 60.818468\n",
      "epoch 374, loss 60.770791\n",
      "epoch 375, loss 60.723358\n",
      "epoch 376, loss 60.676192\n",
      "epoch 377, loss 60.629274\n",
      "epoch 378, loss 60.582617\n",
      "epoch 379, loss 60.536203\n",
      "epoch 380, loss 60.490044\n",
      "epoch 381, loss 60.444112\n",
      "epoch 382, loss 60.398420\n",
      "epoch 383, loss 60.353017\n",
      "epoch 384, loss 60.307815\n",
      "epoch 385, loss 60.262857\n",
      "epoch 386, loss 60.218144\n",
      "epoch 387, loss 60.173669\n",
      "epoch 388, loss 60.129423\n",
      "epoch 389, loss 60.085432\n",
      "epoch 390, loss 60.041667\n",
      "epoch 391, loss 59.998121\n",
      "epoch 392, loss 59.954815\n",
      "epoch 393, loss 59.911714\n",
      "epoch 394, loss 59.868864\n",
      "epoch 395, loss 59.826226\n",
      "epoch 396, loss 59.783818\n",
      "epoch 397, loss 59.741635\n",
      "epoch 398, loss 59.699678\n",
      "epoch 399, loss 59.657912\n",
      "epoch 400, loss 59.616395\n",
      "epoch 401, loss 59.575073\n",
      "epoch 402, loss 59.533987\n",
      "epoch 403, loss 59.493098\n",
      "epoch 404, loss 59.452442\n",
      "epoch 405, loss 59.411974\n",
      "epoch 406, loss 59.371738\n",
      "epoch 407, loss 59.331700\n",
      "epoch 408, loss 59.291872\n",
      "epoch 409, loss 59.252246\n",
      "epoch 410, loss 59.212826\n",
      "epoch 411, loss 59.173605\n",
      "epoch 412, loss 59.134600\n",
      "epoch 413, loss 59.095808\n",
      "epoch 414, loss 59.057186\n",
      "epoch 415, loss 59.018787\n",
      "epoch 416, loss 58.980566\n",
      "epoch 417, loss 58.942555\n",
      "epoch 418, loss 58.904733\n",
      "epoch 419, loss 58.867110\n",
      "epoch 420, loss 58.829670\n",
      "epoch 421, loss 58.792428\n",
      "epoch 422, loss 58.755377\n",
      "epoch 423, loss 58.718517\n",
      "epoch 424, loss 58.681844\n",
      "epoch 425, loss 58.645362\n",
      "epoch 426, loss 58.609060\n",
      "epoch 427, loss 58.572944\n",
      "epoch 428, loss 58.537000\n",
      "epoch 429, loss 58.501241\n",
      "epoch 430, loss 58.465660\n",
      "epoch 431, loss 58.430267\n",
      "epoch 432, loss 58.395061\n",
      "epoch 433, loss 58.360019\n",
      "epoch 434, loss 58.325174\n",
      "epoch 435, loss 58.290469\n",
      "epoch 436, loss 58.255974\n",
      "epoch 437, loss 58.221633\n",
      "epoch 438, loss 58.187451\n",
      "epoch 439, loss 58.153456\n",
      "epoch 440, loss 58.119648\n",
      "epoch 441, loss 58.085988\n",
      "epoch 442, loss 58.052485\n",
      "epoch 443, loss 58.019172\n",
      "epoch 444, loss 57.986013\n",
      "epoch 445, loss 57.953018\n",
      "epoch 446, loss 57.920202\n",
      "epoch 447, loss 57.887531\n",
      "epoch 448, loss 57.855034\n",
      "epoch 449, loss 57.822687\n",
      "epoch 450, loss 57.790509\n",
      "epoch 451, loss 57.758475\n",
      "epoch 452, loss 57.726607\n",
      "epoch 453, loss 57.694916\n",
      "epoch 454, loss 57.663332\n",
      "epoch 455, loss 57.631956\n",
      "epoch 456, loss 57.600701\n",
      "epoch 457, loss 57.569593\n",
      "epoch 458, loss 57.538670\n",
      "epoch 459, loss 57.507878\n",
      "epoch 460, loss 57.477249\n",
      "epoch 461, loss 57.446744\n",
      "epoch 462, loss 57.416394\n",
      "epoch 463, loss 57.386199\n",
      "epoch 464, loss 57.356166\n",
      "epoch 465, loss 57.326265\n",
      "epoch 466, loss 57.296524\n",
      "epoch 467, loss 57.266906\n",
      "epoch 468, loss 57.237433\n",
      "epoch 469, loss 57.208106\n",
      "epoch 470, loss 57.178916\n",
      "epoch 471, loss 57.149889\n",
      "epoch 472, loss 57.120985\n",
      "epoch 473, loss 57.092216\n",
      "epoch 474, loss 57.063590\n",
      "epoch 475, loss 57.035096\n",
      "epoch 476, loss 57.006721\n",
      "epoch 477, loss 56.978495\n",
      "epoch 478, loss 56.950416\n",
      "epoch 479, loss 56.922458\n",
      "epoch 480, loss 56.894632\n",
      "epoch 481, loss 56.866965\n",
      "epoch 482, loss 56.839403\n",
      "epoch 483, loss 56.811978\n",
      "epoch 484, loss 56.784677\n",
      "epoch 485, loss 56.757528\n",
      "epoch 486, loss 56.730500\n",
      "epoch 487, loss 56.703589\n",
      "epoch 488, loss 56.676798\n",
      "epoch 489, loss 56.650145\n",
      "epoch 490, loss 56.623616\n",
      "epoch 491, loss 56.597191\n",
      "epoch 492, loss 56.570925\n",
      "epoch 493, loss 56.544750\n",
      "epoch 494, loss 56.518708\n",
      "epoch 495, loss 56.492790\n",
      "epoch 496, loss 56.467008\n",
      "epoch 497, loss 56.441324\n",
      "epoch 498, loss 56.415756\n",
      "epoch 499, loss 56.390314\n",
      "epoch 500, loss 56.365006\n",
      "epoch 501, loss 56.339788\n",
      "epoch 502, loss 56.314698\n",
      "epoch 503, loss 56.289725\n",
      "epoch 504, loss 56.264858\n",
      "epoch 505, loss 56.240114\n",
      "epoch 506, loss 56.215474\n",
      "epoch 507, loss 56.190971\n",
      "epoch 508, loss 56.166564\n",
      "epoch 509, loss 56.142283\n",
      "epoch 510, loss 56.118094\n",
      "epoch 511, loss 56.094005\n",
      "epoch 512, loss 56.070060\n",
      "epoch 513, loss 56.046186\n",
      "epoch 514, loss 56.022446\n",
      "epoch 515, loss 55.998805\n",
      "epoch 516, loss 55.975269\n",
      "epoch 517, loss 55.951846\n",
      "epoch 518, loss 55.928534\n",
      "epoch 519, loss 55.905301\n",
      "epoch 520, loss 55.882193\n",
      "epoch 521, loss 55.859196\n",
      "epoch 522, loss 55.836301\n",
      "epoch 523, loss 55.813473\n",
      "epoch 524, loss 55.790783\n",
      "epoch 525, loss 55.768196\n",
      "epoch 526, loss 55.745703\n",
      "epoch 527, loss 55.723302\n",
      "epoch 528, loss 55.700990\n",
      "epoch 529, loss 55.678793\n",
      "epoch 530, loss 55.656682\n",
      "epoch 531, loss 55.634681\n",
      "epoch 532, loss 55.612761\n",
      "epoch 533, loss 55.590962\n",
      "epoch 534, loss 55.569256\n",
      "epoch 535, loss 55.547628\n",
      "epoch 536, loss 55.526113\n",
      "epoch 537, loss 55.504673\n",
      "epoch 538, loss 55.483352\n",
      "epoch 539, loss 55.462113\n",
      "epoch 540, loss 55.440960\n",
      "epoch 541, loss 55.419881\n",
      "epoch 542, loss 55.398928\n",
      "epoch 543, loss 55.378062\n",
      "epoch 544, loss 55.357264\n",
      "epoch 545, loss 55.336578\n",
      "epoch 546, loss 55.315970\n",
      "epoch 547, loss 55.295451\n",
      "epoch 548, loss 55.275030\n",
      "epoch 549, loss 55.254688\n",
      "epoch 550, loss 55.234423\n",
      "epoch 551, loss 55.214271\n",
      "epoch 552, loss 55.194193\n",
      "epoch 553, loss 55.174194\n",
      "epoch 554, loss 55.154290\n",
      "epoch 555, loss 55.134458\n",
      "epoch 556, loss 55.114731\n",
      "epoch 557, loss 55.095069\n",
      "epoch 558, loss 55.075504\n",
      "epoch 559, loss 55.056015\n",
      "epoch 560, loss 55.036610\n",
      "epoch 561, loss 55.017279\n",
      "epoch 562, loss 54.998047\n",
      "epoch 563, loss 54.978890\n",
      "epoch 564, loss 54.959790\n",
      "epoch 565, loss 54.940810\n",
      "epoch 566, loss 54.921872\n",
      "epoch 567, loss 54.903040\n",
      "epoch 568, loss 54.884281\n",
      "epoch 569, loss 54.865593\n",
      "epoch 570, loss 54.846987\n",
      "epoch 571, loss 54.828464\n",
      "epoch 572, loss 54.809996\n",
      "epoch 573, loss 54.791630\n",
      "epoch 574, loss 54.773318\n",
      "epoch 575, loss 54.755098\n",
      "epoch 576, loss 54.736954\n",
      "epoch 577, loss 54.718876\n",
      "epoch 578, loss 54.700860\n",
      "epoch 579, loss 54.682958\n",
      "epoch 580, loss 54.665104\n",
      "epoch 581, loss 54.647324\n",
      "epoch 582, loss 54.629605\n",
      "epoch 583, loss 54.611960\n",
      "epoch 584, loss 54.594381\n",
      "epoch 585, loss 54.576901\n",
      "epoch 586, loss 54.559476\n",
      "epoch 587, loss 54.542110\n",
      "epoch 588, loss 54.524837\n",
      "epoch 589, loss 54.507605\n",
      "epoch 590, loss 54.490478\n",
      "epoch 591, loss 54.473384\n",
      "epoch 592, loss 54.456378\n",
      "epoch 593, loss 54.439437\n",
      "epoch 594, loss 54.422549\n",
      "epoch 595, loss 54.405765\n",
      "epoch 596, loss 54.389015\n",
      "epoch 597, loss 54.372351\n",
      "epoch 598, loss 54.355727\n",
      "epoch 599, loss 54.339194\n",
      "epoch 600, loss 54.322718\n",
      "epoch 601, loss 54.306312\n",
      "epoch 602, loss 54.289956\n",
      "epoch 603, loss 54.273669\n",
      "epoch 604, loss 54.257461\n",
      "epoch 605, loss 54.241321\n",
      "epoch 606, loss 54.225217\n",
      "epoch 607, loss 54.209199\n",
      "epoch 608, loss 54.193234\n",
      "epoch 609, loss 54.177320\n",
      "epoch 610, loss 54.161493\n",
      "epoch 611, loss 54.145706\n",
      "epoch 612, loss 54.129988\n",
      "epoch 613, loss 54.114323\n",
      "epoch 614, loss 54.098728\n",
      "epoch 615, loss 54.083179\n",
      "epoch 616, loss 54.067710\n",
      "epoch 617, loss 54.052279\n",
      "epoch 618, loss 54.036933\n",
      "epoch 619, loss 54.021648\n",
      "epoch 620, loss 54.006395\n",
      "epoch 621, loss 53.991186\n",
      "epoch 622, loss 53.976081\n",
      "epoch 623, loss 53.960996\n",
      "epoch 624, loss 53.945981\n",
      "epoch 625, loss 53.931029\n",
      "epoch 626, loss 53.916124\n",
      "epoch 627, loss 53.901272\n",
      "epoch 628, loss 53.886469\n",
      "epoch 629, loss 53.871750\n",
      "epoch 630, loss 53.857066\n",
      "epoch 631, loss 53.842427\n",
      "epoch 632, loss 53.827871\n",
      "epoch 633, loss 53.813348\n",
      "epoch 634, loss 53.798884\n",
      "epoch 635, loss 53.784472\n",
      "epoch 636, loss 53.770103\n",
      "epoch 637, loss 53.755805\n",
      "epoch 638, loss 53.741566\n",
      "epoch 639, loss 53.727353\n",
      "epoch 640, loss 53.713228\n",
      "epoch 641, loss 53.699127\n",
      "epoch 642, loss 53.685081\n",
      "epoch 643, loss 53.671092\n",
      "epoch 644, loss 53.657164\n",
      "epoch 645, loss 53.643251\n",
      "epoch 646, loss 53.629413\n",
      "epoch 647, loss 53.615630\n",
      "epoch 648, loss 53.601895\n",
      "epoch 649, loss 53.588209\n",
      "epoch 650, loss 53.574558\n",
      "epoch 651, loss 53.560944\n",
      "epoch 652, loss 53.547400\n",
      "epoch 653, loss 53.533908\n",
      "epoch 654, loss 53.520444\n",
      "epoch 655, loss 53.507046\n",
      "epoch 656, loss 53.493694\n",
      "epoch 657, loss 53.480384\n",
      "epoch 658, loss 53.467115\n",
      "epoch 659, loss 53.453893\n",
      "epoch 660, loss 53.440724\n",
      "epoch 661, loss 53.427609\n",
      "epoch 662, loss 53.414540\n",
      "epoch 663, loss 53.401503\n",
      "epoch 664, loss 53.388535\n",
      "epoch 665, loss 53.375585\n",
      "epoch 666, loss 53.362699\n",
      "epoch 667, loss 53.349843\n",
      "epoch 668, loss 53.337040\n",
      "epoch 669, loss 53.324282\n",
      "epoch 670, loss 53.311555\n",
      "epoch 671, loss 53.298895\n",
      "epoch 672, loss 53.286259\n",
      "epoch 673, loss 53.273686\n",
      "epoch 674, loss 53.261130\n",
      "epoch 675, loss 53.248629\n",
      "epoch 676, loss 53.236185\n",
      "epoch 677, loss 53.223764\n",
      "epoch 678, loss 53.211387\n",
      "epoch 679, loss 53.199065\n",
      "epoch 680, loss 53.186778\n",
      "epoch 681, loss 53.174520\n",
      "epoch 682, loss 53.162305\n",
      "epoch 683, loss 53.150130\n",
      "epoch 684, loss 53.138010\n",
      "epoch 685, loss 53.125925\n",
      "epoch 686, loss 53.113882\n",
      "epoch 687, loss 53.101865\n",
      "epoch 688, loss 53.089923\n",
      "epoch 689, loss 53.077982\n",
      "epoch 690, loss 53.066088\n",
      "epoch 691, loss 53.054237\n",
      "epoch 692, loss 53.042429\n",
      "epoch 693, loss 53.030653\n",
      "epoch 694, loss 53.018942\n",
      "epoch 695, loss 53.007231\n",
      "epoch 696, loss 52.995565\n",
      "epoch 697, loss 52.983960\n",
      "epoch 698, loss 52.972375\n",
      "epoch 699, loss 52.960833\n",
      "epoch 700, loss 52.949330\n",
      "epoch 701, loss 52.937853\n",
      "epoch 702, loss 52.926436\n",
      "epoch 703, loss 52.915042\n",
      "epoch 704, loss 52.903675\n",
      "epoch 705, loss 52.892361\n",
      "epoch 706, loss 52.881065\n",
      "epoch 707, loss 52.869821\n",
      "epoch 708, loss 52.858607\n",
      "epoch 709, loss 52.847416\n",
      "epoch 710, loss 52.836284\n",
      "epoch 711, loss 52.825173\n",
      "epoch 712, loss 52.814098\n",
      "epoch 713, loss 52.803061\n",
      "epoch 714, loss 52.792060\n",
      "epoch 715, loss 52.781060\n",
      "epoch 716, loss 52.770122\n",
      "epoch 717, loss 52.759231\n",
      "epoch 718, loss 52.748352\n",
      "epoch 719, loss 52.737532\n",
      "epoch 720, loss 52.726722\n",
      "epoch 721, loss 52.715951\n",
      "epoch 722, loss 52.705208\n",
      "epoch 723, loss 52.694498\n",
      "epoch 724, loss 52.683833\n",
      "epoch 725, loss 52.673205\n",
      "epoch 726, loss 52.662592\n",
      "epoch 727, loss 52.652023\n",
      "epoch 728, loss 52.641474\n",
      "epoch 729, loss 52.630973\n",
      "epoch 730, loss 52.620504\n",
      "epoch 731, loss 52.610057\n",
      "epoch 732, loss 52.599643\n",
      "epoch 733, loss 52.589256\n",
      "epoch 734, loss 52.578914\n",
      "epoch 735, loss 52.568587\n",
      "epoch 736, loss 52.558298\n",
      "epoch 737, loss 52.548020\n",
      "epoch 738, loss 52.537779\n",
      "epoch 739, loss 52.527591\n",
      "epoch 740, loss 52.517404\n",
      "epoch 741, loss 52.507277\n",
      "epoch 742, loss 52.497168\n",
      "epoch 743, loss 52.487083\n",
      "epoch 744, loss 52.477009\n",
      "epoch 745, loss 52.467005\n",
      "epoch 746, loss 52.457005\n",
      "epoch 747, loss 52.447042\n",
      "epoch 748, loss 52.437114\n",
      "epoch 749, loss 52.427208\n",
      "epoch 750, loss 52.417316\n",
      "epoch 751, loss 52.407460\n",
      "epoch 752, loss 52.397631\n",
      "epoch 753, loss 52.387839\n",
      "epoch 754, loss 52.378071\n",
      "epoch 755, loss 52.368338\n",
      "epoch 756, loss 52.358626\n",
      "epoch 757, loss 52.348948\n",
      "epoch 758, loss 52.339284\n",
      "epoch 759, loss 52.329643\n",
      "epoch 760, loss 52.320039\n",
      "epoch 761, loss 52.310462\n",
      "epoch 762, loss 52.300905\n",
      "epoch 763, loss 52.291384\n",
      "epoch 764, loss 52.281875\n",
      "epoch 765, loss 52.272419\n",
      "epoch 766, loss 52.262979\n",
      "epoch 767, loss 52.253564\n",
      "epoch 768, loss 52.244164\n",
      "epoch 769, loss 52.234798\n",
      "epoch 770, loss 52.225452\n",
      "epoch 771, loss 52.216141\n",
      "epoch 772, loss 52.206840\n",
      "epoch 773, loss 52.197579\n",
      "epoch 774, loss 52.188334\n",
      "epoch 775, loss 52.179114\n",
      "epoch 776, loss 52.169915\n",
      "epoch 777, loss 52.160738\n",
      "epoch 778, loss 52.151606\n",
      "epoch 779, loss 52.142471\n",
      "epoch 780, loss 52.133384\n",
      "epoch 781, loss 52.124310\n",
      "epoch 782, loss 52.115261\n",
      "epoch 783, loss 52.106242\n",
      "epoch 784, loss 52.097236\n",
      "epoch 785, loss 52.088273\n",
      "epoch 786, loss 52.079325\n",
      "epoch 787, loss 52.070397\n",
      "epoch 788, loss 52.061479\n",
      "epoch 789, loss 52.052595\n",
      "epoch 790, loss 52.043742\n",
      "epoch 791, loss 52.034898\n",
      "epoch 792, loss 52.026089\n",
      "epoch 793, loss 52.017294\n",
      "epoch 794, loss 52.008532\n",
      "epoch 795, loss 51.999785\n",
      "epoch 796, loss 51.991062\n",
      "epoch 797, loss 51.982364\n",
      "epoch 798, loss 51.973688\n",
      "epoch 799, loss 51.965025\n",
      "epoch 800, loss 51.956380\n",
      "epoch 801, loss 51.947769\n",
      "epoch 802, loss 51.939178\n",
      "epoch 803, loss 51.930619\n",
      "epoch 804, loss 51.922071\n",
      "epoch 805, loss 51.913541\n",
      "epoch 806, loss 51.905027\n",
      "epoch 807, loss 51.896535\n",
      "epoch 808, loss 51.888074\n",
      "epoch 809, loss 51.879623\n",
      "epoch 810, loss 51.871200\n",
      "epoch 811, loss 51.862821\n",
      "epoch 812, loss 51.854428\n",
      "epoch 813, loss 51.846071\n",
      "epoch 814, loss 51.837720\n",
      "epoch 815, loss 51.829405\n",
      "epoch 816, loss 51.821101\n",
      "epoch 817, loss 51.812806\n",
      "epoch 818, loss 51.804553\n",
      "epoch 819, loss 51.796325\n",
      "epoch 820, loss 51.788105\n",
      "epoch 821, loss 51.779906\n",
      "epoch 822, loss 51.771740\n",
      "epoch 823, loss 51.763581\n",
      "epoch 824, loss 51.755421\n",
      "epoch 825, loss 51.747309\n",
      "epoch 826, loss 51.739201\n",
      "epoch 827, loss 51.731116\n",
      "epoch 828, loss 51.723047\n",
      "epoch 829, loss 51.715006\n",
      "epoch 830, loss 51.706967\n",
      "epoch 831, loss 51.698953\n",
      "epoch 832, loss 51.690960\n",
      "epoch 833, loss 51.682977\n",
      "epoch 834, loss 51.675028\n",
      "epoch 835, loss 51.667083\n",
      "epoch 836, loss 51.659157\n",
      "epoch 837, loss 51.651273\n",
      "epoch 838, loss 51.643383\n",
      "epoch 839, loss 51.635518\n",
      "epoch 840, loss 51.627682\n",
      "epoch 841, loss 51.619847\n",
      "epoch 842, loss 51.612030\n",
      "epoch 843, loss 51.604236\n",
      "epoch 844, loss 51.596454\n",
      "epoch 845, loss 51.588700\n",
      "epoch 846, loss 51.580948\n",
      "epoch 847, loss 51.573222\n",
      "epoch 848, loss 51.565516\n",
      "epoch 849, loss 51.557825\n",
      "epoch 850, loss 51.550146\n",
      "epoch 851, loss 51.542494\n",
      "epoch 852, loss 51.534843\n",
      "epoch 853, loss 51.527219\n",
      "epoch 854, loss 51.519623\n",
      "epoch 855, loss 51.512027\n",
      "epoch 856, loss 51.504452\n",
      "epoch 857, loss 51.496916\n",
      "epoch 858, loss 51.489364\n",
      "epoch 859, loss 51.481838\n",
      "epoch 860, loss 51.474314\n",
      "epoch 861, loss 51.466827\n",
      "epoch 862, loss 51.459345\n",
      "epoch 863, loss 51.451874\n",
      "epoch 864, loss 51.444430\n",
      "epoch 865, loss 51.437001\n",
      "epoch 866, loss 51.429573\n",
      "epoch 867, loss 51.422196\n",
      "epoch 868, loss 51.414807\n",
      "epoch 869, loss 51.407447\n",
      "epoch 870, loss 51.400075\n",
      "epoch 871, loss 51.392755\n",
      "epoch 872, loss 51.385404\n",
      "epoch 873, loss 51.378115\n",
      "epoch 874, loss 51.370806\n",
      "epoch 875, loss 51.363550\n",
      "epoch 876, loss 51.356277\n",
      "epoch 877, loss 51.349022\n",
      "epoch 878, loss 51.341784\n",
      "epoch 879, loss 51.334576\n",
      "epoch 880, loss 51.327378\n",
      "epoch 881, loss 51.320186\n",
      "epoch 882, loss 51.313001\n",
      "epoch 883, loss 51.305852\n",
      "epoch 884, loss 51.298698\n",
      "epoch 885, loss 51.291571\n",
      "epoch 886, loss 51.284443\n",
      "epoch 887, loss 51.277355\n",
      "epoch 888, loss 51.270254\n",
      "epoch 889, loss 51.263178\n",
      "epoch 890, loss 51.256119\n",
      "epoch 891, loss 51.249056\n",
      "epoch 892, loss 51.242025\n",
      "epoch 893, loss 51.235011\n",
      "epoch 894, loss 51.228003\n",
      "epoch 895, loss 51.220996\n",
      "epoch 896, loss 51.214017\n",
      "epoch 897, loss 51.207054\n",
      "epoch 898, loss 51.200094\n",
      "epoch 899, loss 51.193142\n",
      "epoch 900, loss 51.186228\n",
      "epoch 901, loss 51.179306\n",
      "epoch 902, loss 51.172408\n",
      "epoch 903, loss 51.165518\n",
      "epoch 904, loss 51.158642\n",
      "epoch 905, loss 51.151786\n",
      "epoch 906, loss 51.144925\n",
      "epoch 907, loss 51.138083\n",
      "epoch 908, loss 51.131249\n",
      "epoch 909, loss 51.124447\n",
      "epoch 910, loss 51.117646\n",
      "epoch 911, loss 51.110867\n",
      "epoch 912, loss 51.104089\n",
      "epoch 913, loss 51.097326\n",
      "epoch 914, loss 51.090569\n",
      "epoch 915, loss 51.083849\n",
      "epoch 916, loss 51.077112\n",
      "epoch 917, loss 51.070413\n",
      "epoch 918, loss 51.063691\n",
      "epoch 919, loss 51.057004\n",
      "epoch 920, loss 51.050343\n",
      "epoch 921, loss 51.043674\n",
      "epoch 922, loss 51.037008\n",
      "epoch 923, loss 51.030371\n",
      "epoch 924, loss 51.023742\n",
      "epoch 925, loss 51.017128\n",
      "epoch 926, loss 51.010506\n",
      "epoch 927, loss 51.003924\n",
      "epoch 928, loss 50.997347\n",
      "epoch 929, loss 50.990772\n",
      "epoch 930, loss 50.984228\n",
      "epoch 931, loss 50.977668\n",
      "epoch 932, loss 50.971138\n",
      "epoch 933, loss 50.964592\n",
      "epoch 934, loss 50.958069\n",
      "epoch 935, loss 50.951584\n",
      "epoch 936, loss 50.945083\n",
      "epoch 937, loss 50.938600\n",
      "epoch 938, loss 50.932127\n",
      "epoch 939, loss 50.925668\n",
      "epoch 940, loss 50.919221\n",
      "epoch 941, loss 50.912783\n",
      "epoch 942, loss 50.906349\n",
      "epoch 943, loss 50.899923\n",
      "epoch 944, loss 50.893527\n",
      "epoch 945, loss 50.887126\n",
      "epoch 946, loss 50.880734\n",
      "epoch 947, loss 50.874368\n",
      "epoch 948, loss 50.867996\n",
      "epoch 949, loss 50.861658\n",
      "epoch 950, loss 50.855297\n",
      "epoch 951, loss 50.848968\n",
      "epoch 952, loss 50.842639\n",
      "epoch 953, loss 50.836331\n",
      "epoch 954, loss 50.830037\n",
      "epoch 955, loss 50.823727\n",
      "epoch 956, loss 50.817459\n",
      "epoch 957, loss 50.811185\n",
      "epoch 958, loss 50.804923\n",
      "epoch 959, loss 50.798683\n",
      "epoch 960, loss 50.792424\n",
      "epoch 961, loss 50.786196\n",
      "epoch 962, loss 50.779975\n",
      "epoch 963, loss 50.773756\n",
      "epoch 964, loss 50.767560\n",
      "epoch 965, loss 50.761380\n",
      "epoch 966, loss 50.755190\n",
      "epoch 967, loss 50.749025\n",
      "epoch 968, loss 50.742849\n",
      "epoch 969, loss 50.736706\n",
      "epoch 970, loss 50.730553\n",
      "epoch 971, loss 50.724411\n",
      "epoch 972, loss 50.718294\n",
      "epoch 973, loss 50.712179\n",
      "epoch 974, loss 50.706083\n",
      "epoch 975, loss 50.699973\n",
      "epoch 976, loss 50.693878\n",
      "epoch 977, loss 50.687799\n",
      "epoch 978, loss 50.681716\n",
      "epoch 979, loss 50.675659\n",
      "epoch 980, loss 50.669604\n",
      "epoch 981, loss 50.663551\n",
      "epoch 982, loss 50.657512\n",
      "epoch 983, loss 50.651495\n",
      "epoch 984, loss 50.645484\n",
      "epoch 985, loss 50.639489\n",
      "epoch 986, loss 50.633484\n",
      "epoch 987, loss 50.627484\n",
      "epoch 988, loss 50.621515\n",
      "epoch 989, loss 50.615542\n",
      "epoch 990, loss 50.609563\n",
      "epoch 991, loss 50.603637\n",
      "epoch 992, loss 50.597682\n",
      "epoch 993, loss 50.591741\n",
      "epoch 994, loss 50.585808\n",
      "epoch 995, loss 50.579874\n",
      "epoch 996, loss 50.573970\n",
      "epoch 997, loss 50.568074\n",
      "epoch 998, loss 50.562168\n",
      "epoch 999, loss 50.556290\n",
      "epoch 1000, loss 50.550405\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for X, y in data_iter(batch_size, X_train, y_train):   \n",
    "        \n",
    "        \n",
    "        l = squared_loss(linreg(X,w,b), y)\n",
    "        l.backward()\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j in range(len(w)):\n",
    "                w[j] -= lr * w.grad[j]\n",
    "            b -= lr * b.grad\n",
    "            \n",
    "            # Manually zero the gradients after updating weights\n",
    "            w.grad=None\n",
    "            b.grad=None\n",
    "\n",
    "    train_l = squared_loss(linreg(X_train, w, b), y_train)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1229,  0.1241,  0.0019, -1.1362, -0.9047,  1.0291,  0.0901, -0.1621,\n",
       "          0.1455, -0.0123,  0.5093,  0.0288, -0.7253], requires_grad=True),\n",
       " tensor([0.1201], requires_grad=True))"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(55.3358, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_loss(linreg(X_test,w,b), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
