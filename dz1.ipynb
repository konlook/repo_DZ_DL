{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston Housing Dataset\n",
    "\n",
    "\n",
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n",
    "\n",
    "\n",
    "CRIM - per capita crime rate by town\n",
    "\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS - proportion of non-retail business acres per town.\n",
    "\n",
    "CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM - average number of rooms per dwelling \n",
    "\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "RAD - index of accessibility to radial highways\n",
    "\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "LSTAT - % lower status of the population\n",
    "\n",
    "MEDV - Median value of owner-occupied homes in $1000's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "                 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df=pd.read_csv('BostonHousPrice\\housing.csv',sep=r'\\s+', names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    int64  \n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    int64  \n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 59.3 KB\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: MEDV, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=pd.Series(df['MEDV'])\n",
    "del(df['MEDV'])\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "arr_df=np.array(df)\n",
    "arr_y=np.array(Y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(arr_df, arr_y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.double)\n",
    "X_test = torch.tensor(X_test, dtype=torch.double)\n",
    "y_train = torch.tensor(y_train, dtype=torch.double)\n",
    "y_test = torch.tensor(y_test, dtype=torch.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train),\n",
    "      type(X_test),\n",
    "      type(y_train),\n",
    "      type(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=torch.Size([379, 13])\n",
      "X_test.shape= torch.Size([127, 13])\n",
      "y_train.shape=torch.Size([379])\n",
      "y_test.shape= torch.Size([127])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'X_train.shape={X_train.shape}\\n'\n",
    "      f'X_test.shape= {X_test.shape}\\n'\n",
    "      f'y_train.shape={y_train.shape}\\n'\n",
    "      f'y_test.shape= {y_test.shape}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    #random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = indices[i: min(i + batch_size, num_examples)]\n",
    "        yield features[j, :], labels[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5270e-02, 0.0000e+00, 1.1930e+01, 0.0000e+00, 5.7300e-01, 6.1200e+00,\n",
      "         7.6700e+01, 2.2875e+00, 1.0000e+00, 2.7300e+02, 2.1000e+01, 3.9690e+02,\n",
      "         9.0800e+00],\n",
      "        [1.3914e-01, 0.0000e+00, 4.0500e+00, 0.0000e+00, 5.1000e-01, 5.5720e+00,\n",
      "         8.8500e+01, 2.5961e+00, 5.0000e+00, 2.9600e+02, 1.6600e+01, 3.9690e+02,\n",
      "         1.4690e+01],\n",
      "        [4.1130e-02, 2.5000e+01, 4.8600e+00, 0.0000e+00, 4.2600e-01, 6.7270e+00,\n",
      "         3.3500e+01, 5.4007e+00, 4.0000e+00, 2.8100e+02, 1.9000e+01, 3.9690e+02,\n",
      "         5.2900e+00],\n",
      "        [1.8836e-01, 0.0000e+00, 6.9100e+00, 0.0000e+00, 4.4800e-01, 5.7860e+00,\n",
      "         3.3300e+01, 5.1004e+00, 3.0000e+00, 2.3300e+02, 1.7900e+01, 3.9690e+02,\n",
      "         1.4150e+01],\n",
      "        [4.0202e-01, 0.0000e+00, 9.9000e+00, 0.0000e+00, 5.4400e-01, 6.3820e+00,\n",
      "         6.7200e+01, 3.5325e+00, 4.0000e+00, 3.0400e+02, 1.8400e+01, 3.9521e+02,\n",
      "         1.0360e+01],\n",
      "        [2.8750e-02, 2.8000e+01, 1.5040e+01, 0.0000e+00, 4.6400e-01, 6.2110e+00,\n",
      "         2.8900e+01, 3.6659e+00, 4.0000e+00, 2.7000e+02, 1.8200e+01, 3.9633e+02,\n",
      "         6.2100e+00],\n",
      "        [1.1578e+01, 0.0000e+00, 1.8100e+01, 0.0000e+00, 7.0000e-01, 5.0360e+00,\n",
      "         9.7000e+01, 1.7700e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n",
      "         2.5680e+01],\n",
      "        [4.4620e-02, 2.5000e+01, 4.8600e+00, 0.0000e+00, 4.2600e-01, 6.6190e+00,\n",
      "         7.0400e+01, 5.4007e+00, 4.0000e+00, 2.8100e+02, 1.9000e+01, 3.9563e+02,\n",
      "         7.2200e+00],\n",
      "        [5.5150e-02, 3.3000e+01, 2.1800e+00, 0.0000e+00, 4.7200e-01, 7.2360e+00,\n",
      "         4.1100e+01, 4.0220e+00, 7.0000e+00, 2.2200e+02, 1.8400e+01, 3.9368e+02,\n",
      "         6.9300e+00],\n",
      "        [3.3211e+00, 0.0000e+00, 1.9580e+01, 1.0000e+00, 8.7100e-01, 5.4030e+00,\n",
      "         1.0000e+02, 1.3216e+00, 5.0000e+00, 4.0300e+02, 1.4700e+01, 3.9690e+02,\n",
      "         2.6820e+01]], dtype=torch.float64) \n",
      " tensor([20.6000, 23.1000, 28.0000, 20.0000, 23.1000, 25.0000,  9.7000, 23.9000,\n",
      "        36.1000, 13.4000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, X_train, y_train):\n",
    "    print(X,'\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn((len(X_train[0])))\n",
    "b = torch.zeros((1,))\n",
    "w.requires_grad_()\n",
    "b.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w1, b1):\n",
    "  return torch.mv(X.float(),w1.float())+b1.float()\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "  return ((y_hat-y.reshape(y_hat.shape)) ** 2).mean()\n",
    "\n",
    "def sgd(params, lr):\n",
    "  for param in params:\n",
    "    param.data[:] = param - lr*param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.8876,  1.3024, -0.9624, -1.2318,  0.7914,  2.1721,  0.1169,  1.0738,\n",
       "         -1.3630, -1.3525,  1.3356, -0.2175, -2.3502], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-6  \n",
    "num_epochs = 1000\n",
    "batch_size=3\n",
    "\n",
    "w = torch.randn((len(X_train[0])))\n",
    "b = torch.zeros((1,))\n",
    "w.requires_grad_()\n",
    "b.requires_grad_()\n",
    "\n",
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1168.048736\n",
      "epoch 2, loss 977.499902\n",
      "epoch 3, loss 831.099494\n",
      "epoch 4, loss 719.193134\n",
      "epoch 5, loss 632.009574\n",
      "epoch 6, loss 562.868040\n",
      "epoch 7, loss 507.147620\n",
      "epoch 8, loss 461.601533\n",
      "epoch 9, loss 423.911268\n",
      "epoch 10, loss 392.390802\n",
      "epoch 11, loss 365.789593\n",
      "epoch 12, loss 343.163233\n",
      "epoch 13, loss 323.784846\n",
      "epoch 14, loss 307.085136\n",
      "epoch 15, loss 292.611400\n",
      "epoch 16, loss 279.999381\n",
      "epoch 17, loss 268.951827\n",
      "epoch 18, loss 259.224607\n",
      "epoch 19, loss 250.615264\n",
      "epoch 20, loss 242.955191\n",
      "epoch 21, loss 236.103368\n",
      "epoch 22, loss 229.940836\n",
      "epoch 23, loss 224.367913\n",
      "epoch 24, loss 219.299942\n",
      "epoch 25, loss 214.665520\n",
      "epoch 26, loss 210.404216\n",
      "epoch 27, loss 206.464861\n",
      "epoch 28, loss 202.804015\n",
      "epoch 29, loss 199.384805\n",
      "epoch 30, loss 196.175736\n",
      "epoch 31, loss 193.150267\n",
      "epoch 32, loss 190.285861\n",
      "epoch 33, loss 187.563303\n",
      "epoch 34, loss 184.966088\n",
      "epoch 35, loss 182.480277\n",
      "epoch 36, loss 180.093927\n",
      "epoch 37, loss 177.797108\n",
      "epoch 38, loss 175.580924\n",
      "epoch 39, loss 173.438101\n",
      "epoch 40, loss 171.362153\n",
      "epoch 41, loss 169.347620\n",
      "epoch 42, loss 167.389999\n",
      "epoch 43, loss 165.485115\n",
      "epoch 44, loss 163.629359\n",
      "epoch 45, loss 161.819770\n",
      "epoch 46, loss 160.053713\n",
      "epoch 47, loss 158.328948\n",
      "epoch 48, loss 156.643323\n",
      "epoch 49, loss 154.995042\n",
      "epoch 50, loss 153.382401\n",
      "epoch 51, loss 151.804092\n",
      "epoch 52, loss 150.258938\n",
      "epoch 53, loss 148.745529\n",
      "epoch 54, loss 147.262956\n",
      "epoch 55, loss 145.810149\n",
      "epoch 56, loss 144.386289\n",
      "epoch 57, loss 142.990506\n",
      "epoch 58, loss 141.622068\n",
      "epoch 59, loss 140.280194\n",
      "epoch 60, loss 138.964187\n",
      "epoch 61, loss 137.673514\n",
      "epoch 62, loss 136.407462\n",
      "epoch 63, loss 135.165514\n",
      "epoch 64, loss 133.947053\n",
      "epoch 65, loss 132.751515\n",
      "epoch 66, loss 131.578374\n",
      "epoch 67, loss 130.427121\n",
      "epoch 68, loss 129.297326\n",
      "epoch 69, loss 128.188468\n",
      "epoch 70, loss 127.100057\n",
      "epoch 71, loss 126.031645\n",
      "epoch 72, loss 124.982874\n",
      "epoch 73, loss 123.953170\n",
      "epoch 74, loss 122.942218\n",
      "epoch 75, loss 121.949571\n",
      "epoch 76, loss 120.974727\n",
      "epoch 77, loss 120.017573\n",
      "epoch 78, loss 119.077554\n",
      "epoch 79, loss 118.154284\n",
      "epoch 80, loss 117.247448\n",
      "epoch 81, loss 116.356661\n",
      "epoch 82, loss 115.481545\n",
      "epoch 83, loss 114.621750\n",
      "epoch 84, loss 113.777017\n",
      "epoch 85, loss 112.946954\n",
      "epoch 86, loss 112.131256\n",
      "epoch 87, loss 111.329625\n",
      "epoch 88, loss 110.541811\n",
      "epoch 89, loss 109.767405\n",
      "epoch 90, loss 109.006157\n",
      "epoch 91, loss 108.257817\n",
      "epoch 92, loss 107.522077\n",
      "epoch 93, loss 106.798620\n",
      "epoch 94, loss 106.087274\n",
      "epoch 95, loss 105.387716\n",
      "epoch 96, loss 104.699664\n",
      "epoch 97, loss 104.022938\n",
      "epoch 98, loss 103.357253\n",
      "epoch 99, loss 102.702399\n",
      "epoch 100, loss 102.058111\n",
      "epoch 101, loss 101.424159\n",
      "epoch 102, loss 100.800317\n",
      "epoch 103, loss 100.186417\n",
      "epoch 104, loss 99.582230\n",
      "epoch 105, loss 98.987525\n",
      "epoch 106, loss 98.402153\n",
      "epoch 107, loss 97.825879\n",
      "epoch 108, loss 97.258540\n",
      "epoch 109, loss 96.699875\n",
      "epoch 110, loss 96.149767\n",
      "epoch 111, loss 95.608013\n",
      "epoch 112, loss 95.074493\n",
      "epoch 113, loss 94.548984\n",
      "epoch 114, loss 94.031303\n",
      "epoch 115, loss 93.521339\n",
      "epoch 116, loss 93.018886\n",
      "epoch 117, loss 92.523861\n",
      "epoch 118, loss 92.035990\n",
      "epoch 119, loss 91.555219\n",
      "epoch 120, loss 91.081410\n",
      "epoch 121, loss 90.614428\n",
      "epoch 122, loss 90.154059\n",
      "epoch 123, loss 89.700215\n",
      "epoch 124, loss 89.252803\n",
      "epoch 125, loss 88.811618\n",
      "epoch 126, loss 88.376623\n",
      "epoch 127, loss 87.947620\n",
      "epoch 128, loss 87.524556\n",
      "epoch 129, loss 87.107244\n",
      "epoch 130, loss 86.695661\n",
      "epoch 131, loss 86.289585\n",
      "epoch 132, loss 85.889026\n",
      "epoch 133, loss 85.493814\n",
      "epoch 134, loss 85.103828\n",
      "epoch 135, loss 84.718962\n",
      "epoch 136, loss 84.339177\n",
      "epoch 137, loss 83.964370\n",
      "epoch 138, loss 83.594459\n",
      "epoch 139, loss 83.229270\n",
      "epoch 140, loss 82.868795\n",
      "epoch 141, loss 82.512902\n",
      "epoch 142, loss 82.161551\n",
      "epoch 143, loss 81.814601\n",
      "epoch 144, loss 81.472021\n",
      "epoch 145, loss 81.133661\n",
      "epoch 146, loss 80.799556\n",
      "epoch 147, loss 80.469558\n",
      "epoch 148, loss 80.143603\n",
      "epoch 149, loss 79.821634\n",
      "epoch 150, loss 79.503536\n",
      "epoch 151, loss 79.189245\n",
      "epoch 152, loss 78.878770\n",
      "epoch 153, loss 78.571981\n",
      "epoch 154, loss 78.268761\n",
      "epoch 155, loss 77.969192\n",
      "epoch 156, loss 77.673093\n",
      "epoch 157, loss 77.380466\n",
      "epoch 158, loss 77.091223\n",
      "epoch 159, loss 76.805311\n",
      "epoch 160, loss 76.522647\n",
      "epoch 161, loss 76.243231\n",
      "epoch 162, loss 75.966951\n",
      "epoch 163, loss 75.693813\n",
      "epoch 164, loss 75.423701\n",
      "epoch 165, loss 75.156626\n",
      "epoch 166, loss 74.892514\n",
      "epoch 167, loss 74.631288\n",
      "epoch 168, loss 74.372953\n",
      "epoch 169, loss 74.117471\n",
      "epoch 170, loss 73.864725\n",
      "epoch 171, loss 73.614705\n",
      "epoch 172, loss 73.367388\n",
      "epoch 173, loss 73.122702\n",
      "epoch 174, loss 72.880652\n",
      "epoch 175, loss 72.641143\n",
      "epoch 176, loss 72.404174\n",
      "epoch 177, loss 72.169691\n",
      "epoch 178, loss 71.937633\n",
      "epoch 179, loss 71.707995\n",
      "epoch 180, loss 71.480714\n",
      "epoch 181, loss 71.255746\n",
      "epoch 182, loss 71.033082\n",
      "epoch 183, loss 70.812690\n",
      "epoch 184, loss 70.594533\n",
      "epoch 185, loss 70.378572\n",
      "epoch 186, loss 70.164808\n",
      "epoch 187, loss 69.953147\n",
      "epoch 188, loss 69.743602\n",
      "epoch 189, loss 69.536139\n",
      "epoch 190, loss 69.330702\n",
      "epoch 191, loss 69.127262\n",
      "epoch 192, loss 68.925840\n",
      "epoch 193, loss 68.726352\n",
      "epoch 194, loss 68.528833\n",
      "epoch 195, loss 68.333170\n",
      "epoch 196, loss 68.139372\n",
      "epoch 197, loss 67.947459\n",
      "epoch 198, loss 67.757329\n",
      "epoch 199, loss 67.569014\n",
      "epoch 200, loss 67.382477\n",
      "epoch 201, loss 67.197700\n",
      "epoch 202, loss 67.014650\n",
      "epoch 203, loss 66.833290\n",
      "epoch 204, loss 66.653615\n",
      "epoch 205, loss 66.475604\n",
      "epoch 206, loss 66.299208\n",
      "epoch 207, loss 66.124451\n",
      "epoch 208, loss 65.951253\n",
      "epoch 209, loss 65.779633\n",
      "epoch 210, loss 65.609582\n",
      "epoch 211, loss 65.441030\n",
      "epoch 212, loss 65.274027\n",
      "epoch 213, loss 65.108467\n",
      "epoch 214, loss 64.944426\n",
      "epoch 215, loss 64.781822\n",
      "epoch 216, loss 64.620670\n",
      "epoch 217, loss 64.460941\n",
      "epoch 218, loss 64.302586\n",
      "epoch 219, loss 64.145608\n",
      "epoch 220, loss 63.990017\n",
      "epoch 221, loss 63.835760\n",
      "epoch 222, loss 63.682877\n",
      "epoch 223, loss 63.531297\n",
      "epoch 224, loss 63.381001\n",
      "epoch 225, loss 63.231970\n",
      "epoch 226, loss 63.084220\n",
      "epoch 227, loss 62.937744\n",
      "epoch 228, loss 62.792503\n",
      "epoch 229, loss 62.648496\n",
      "epoch 230, loss 62.505679\n",
      "epoch 231, loss 62.364045\n",
      "epoch 232, loss 62.223610\n",
      "epoch 233, loss 62.084357\n",
      "epoch 234, loss 61.946225\n",
      "epoch 235, loss 61.809270\n",
      "epoch 236, loss 61.673426\n",
      "epoch 237, loss 61.538696\n",
      "epoch 238, loss 61.405056\n",
      "epoch 239, loss 61.272483\n",
      "epoch 240, loss 61.141023\n",
      "epoch 241, loss 61.010631\n",
      "epoch 242, loss 60.881286\n",
      "epoch 243, loss 60.752981\n",
      "epoch 244, loss 60.625743\n",
      "epoch 245, loss 60.499469\n",
      "epoch 246, loss 60.374226\n",
      "epoch 247, loss 60.250004\n",
      "epoch 248, loss 60.126744\n",
      "epoch 249, loss 60.004489\n",
      "epoch 250, loss 59.883155\n",
      "epoch 251, loss 59.762807\n",
      "epoch 252, loss 59.643416\n",
      "epoch 253, loss 59.524943\n",
      "epoch 254, loss 59.407412\n",
      "epoch 255, loss 59.290760\n",
      "epoch 256, loss 59.175053\n",
      "epoch 257, loss 59.060183\n",
      "epoch 258, loss 58.946261\n",
      "epoch 259, loss 58.833172\n",
      "epoch 260, loss 58.720984\n",
      "epoch 261, loss 58.609642\n",
      "epoch 262, loss 58.499151\n",
      "epoch 263, loss 58.389507\n",
      "epoch 264, loss 58.280676\n",
      "epoch 265, loss 58.172734\n",
      "epoch 266, loss 58.065555\n",
      "epoch 267, loss 57.959194\n",
      "epoch 268, loss 57.853660\n",
      "epoch 269, loss 57.748883\n",
      "epoch 270, loss 57.644890\n",
      "epoch 271, loss 57.541693\n",
      "epoch 272, loss 57.439280\n",
      "epoch 273, loss 57.337599\n",
      "epoch 274, loss 57.236700\n",
      "epoch 275, loss 57.136544\n",
      "epoch 276, loss 57.037119\n",
      "epoch 277, loss 56.938437\n",
      "epoch 278, loss 56.840481\n",
      "epoch 279, loss 56.743233\n",
      "epoch 280, loss 56.646711\n",
      "epoch 281, loss 56.550899\n",
      "epoch 282, loss 56.455779\n",
      "epoch 283, loss 56.361341\n",
      "epoch 284, loss 56.267574\n",
      "epoch 285, loss 56.174507\n",
      "epoch 286, loss 56.082119\n",
      "epoch 287, loss 55.990401\n",
      "epoch 288, loss 55.899318\n",
      "epoch 289, loss 55.808928\n",
      "epoch 290, loss 55.719184\n",
      "epoch 291, loss 55.630052\n",
      "epoch 292, loss 55.541588\n",
      "epoch 293, loss 55.453755\n",
      "epoch 294, loss 55.366519\n",
      "epoch 295, loss 55.279930\n",
      "epoch 296, loss 55.193964\n",
      "epoch 297, loss 55.108581\n",
      "epoch 298, loss 55.023804\n",
      "epoch 299, loss 54.939653\n",
      "epoch 300, loss 54.856056\n",
      "epoch 301, loss 54.773083\n",
      "epoch 302, loss 54.690660\n",
      "epoch 303, loss 54.608858\n",
      "epoch 304, loss 54.527581\n",
      "epoch 305, loss 54.446887\n",
      "epoch 306, loss 54.366769\n",
      "epoch 307, loss 54.287183\n",
      "epoch 308, loss 54.208166\n",
      "epoch 309, loss 54.129700\n",
      "epoch 310, loss 54.051779\n",
      "epoch 311, loss 53.974394\n",
      "epoch 312, loss 53.897538\n",
      "epoch 313, loss 53.821224\n",
      "epoch 314, loss 53.745433\n",
      "epoch 315, loss 53.670148\n",
      "epoch 316, loss 53.595405\n",
      "epoch 317, loss 53.521139\n",
      "epoch 318, loss 53.447418\n",
      "epoch 319, loss 53.374164\n",
      "epoch 320, loss 53.301432\n",
      "epoch 321, loss 53.229183\n",
      "epoch 322, loss 53.157415\n",
      "epoch 323, loss 53.086147\n",
      "epoch 324, loss 53.015374\n",
      "epoch 325, loss 52.945068\n",
      "epoch 326, loss 52.875233\n",
      "epoch 327, loss 52.805878\n",
      "epoch 328, loss 52.736977\n",
      "epoch 329, loss 52.668545\n",
      "epoch 330, loss 52.600572\n",
      "epoch 331, loss 52.533049\n",
      "epoch 332, loss 52.465981\n",
      "epoch 333, loss 52.399362\n",
      "epoch 334, loss 52.333179\n",
      "epoch 335, loss 52.267436\n",
      "epoch 336, loss 52.202133\n",
      "epoch 337, loss 52.137288\n",
      "epoch 338, loss 52.072880\n",
      "epoch 339, loss 52.008876\n",
      "epoch 340, loss 51.945294\n",
      "epoch 341, loss 51.882137\n",
      "epoch 342, loss 51.819397\n",
      "epoch 343, loss 51.757079\n",
      "epoch 344, loss 51.695161\n",
      "epoch 345, loss 51.633650\n",
      "epoch 346, loss 51.572539\n",
      "epoch 347, loss 51.511816\n",
      "epoch 348, loss 51.451529\n",
      "epoch 349, loss 51.391607\n",
      "epoch 350, loss 51.332098\n",
      "epoch 351, loss 51.272976\n",
      "epoch 352, loss 51.214218\n",
      "epoch 353, loss 51.155870\n",
      "epoch 354, loss 51.097886\n",
      "epoch 355, loss 51.040289\n",
      "epoch 356, loss 50.983068\n",
      "epoch 357, loss 50.926215\n",
      "epoch 358, loss 50.869723\n",
      "epoch 359, loss 50.813597\n",
      "epoch 360, loss 50.757825\n",
      "epoch 361, loss 50.702422\n",
      "epoch 362, loss 50.647377\n",
      "epoch 363, loss 50.592681\n",
      "epoch 364, loss 50.538336\n",
      "epoch 365, loss 50.484350\n",
      "epoch 366, loss 50.430712\n",
      "epoch 367, loss 50.377420\n",
      "epoch 368, loss 50.324457\n",
      "epoch 369, loss 50.271838\n",
      "epoch 370, loss 50.219562\n",
      "epoch 371, loss 50.167620\n",
      "epoch 372, loss 50.116010\n",
      "epoch 373, loss 50.064726\n",
      "epoch 374, loss 50.013769\n",
      "epoch 375, loss 49.963124\n",
      "epoch 376, loss 49.912809\n",
      "epoch 377, loss 49.862813\n",
      "epoch 378, loss 49.813137\n",
      "epoch 379, loss 49.763777\n",
      "epoch 380, loss 49.714714\n",
      "epoch 381, loss 49.665970\n",
      "epoch 382, loss 49.617551\n",
      "epoch 383, loss 49.569433\n",
      "epoch 384, loss 49.521612\n",
      "epoch 385, loss 49.474092\n",
      "epoch 386, loss 49.426874\n",
      "epoch 387, loss 49.379953\n",
      "epoch 388, loss 49.333335\n",
      "epoch 389, loss 49.286976\n",
      "epoch 390, loss 49.240942\n",
      "epoch 391, loss 49.195194\n",
      "epoch 392, loss 49.149724\n",
      "epoch 393, loss 49.104537\n",
      "epoch 394, loss 49.059636\n",
      "epoch 395, loss 49.014996\n",
      "epoch 396, loss 48.970671\n",
      "epoch 397, loss 48.926581\n",
      "epoch 398, loss 48.882803\n",
      "epoch 399, loss 48.839288\n",
      "epoch 400, loss 48.796036\n",
      "epoch 401, loss 48.753036\n",
      "epoch 402, loss 48.710334\n",
      "epoch 403, loss 48.667890\n",
      "epoch 404, loss 48.625698\n",
      "epoch 405, loss 48.583760\n",
      "epoch 406, loss 48.542111\n",
      "epoch 407, loss 48.500699\n",
      "epoch 408, loss 48.459551\n",
      "epoch 409, loss 48.418640\n",
      "epoch 410, loss 48.377986\n",
      "epoch 411, loss 48.337582\n",
      "epoch 412, loss 48.297441\n",
      "epoch 413, loss 48.257521\n",
      "epoch 414, loss 48.217871\n",
      "epoch 415, loss 48.178459\n",
      "epoch 416, loss 48.139295\n",
      "epoch 417, loss 48.100357\n",
      "epoch 418, loss 48.061673\n",
      "epoch 419, loss 48.023212\n",
      "epoch 420, loss 47.984991\n",
      "epoch 421, loss 47.946994\n",
      "epoch 422, loss 47.909250\n",
      "epoch 423, loss 47.871721\n",
      "epoch 424, loss 47.834429\n",
      "epoch 425, loss 47.797360\n",
      "epoch 426, loss 47.760507\n",
      "epoch 427, loss 47.723891\n",
      "epoch 428, loss 47.687497\n",
      "epoch 429, loss 47.651330\n",
      "epoch 430, loss 47.615389\n",
      "epoch 431, loss 47.579656\n",
      "epoch 432, loss 47.544135\n",
      "epoch 433, loss 47.508827\n",
      "epoch 434, loss 47.473734\n",
      "epoch 435, loss 47.438858\n",
      "epoch 436, loss 47.404177\n",
      "epoch 437, loss 47.369725\n",
      "epoch 438, loss 47.335475\n",
      "epoch 439, loss 47.301442\n",
      "epoch 440, loss 47.267592\n",
      "epoch 441, loss 47.233972\n",
      "epoch 442, loss 47.200548\n",
      "epoch 443, loss 47.167312\n",
      "epoch 444, loss 47.134276\n",
      "epoch 445, loss 47.101448\n",
      "epoch 446, loss 47.068814\n",
      "epoch 447, loss 47.036382\n",
      "epoch 448, loss 47.004133\n",
      "epoch 449, loss 46.972100\n",
      "epoch 450, loss 46.940247\n",
      "epoch 451, loss 46.908569\n",
      "epoch 452, loss 46.877089\n",
      "epoch 453, loss 46.845813\n",
      "epoch 454, loss 46.814714\n",
      "epoch 455, loss 46.783782\n",
      "epoch 456, loss 46.753067\n",
      "epoch 457, loss 46.722508\n",
      "epoch 458, loss 46.692147\n",
      "epoch 459, loss 46.661958\n",
      "epoch 460, loss 46.631955\n",
      "epoch 461, loss 46.602117\n",
      "epoch 462, loss 46.572472\n",
      "epoch 463, loss 46.543014\n",
      "epoch 464, loss 46.513710\n",
      "epoch 465, loss 46.484590\n",
      "epoch 466, loss 46.455640\n",
      "epoch 467, loss 46.426855\n",
      "epoch 468, loss 46.398255\n",
      "epoch 469, loss 46.369784\n",
      "epoch 470, loss 46.341522\n",
      "epoch 471, loss 46.313414\n",
      "epoch 472, loss 46.285475\n",
      "epoch 473, loss 46.257703\n",
      "epoch 474, loss 46.230096\n",
      "epoch 475, loss 46.202648\n",
      "epoch 476, loss 46.175360\n",
      "epoch 477, loss 46.148249\n",
      "epoch 478, loss 46.121287\n",
      "epoch 479, loss 46.094487\n",
      "epoch 480, loss 46.067836\n",
      "epoch 481, loss 46.041350\n",
      "epoch 482, loss 46.015014\n",
      "epoch 483, loss 45.988827\n",
      "epoch 484, loss 45.962795\n",
      "epoch 485, loss 45.936925\n",
      "epoch 486, loss 45.911205\n",
      "epoch 487, loss 45.885632\n",
      "epoch 488, loss 45.860222\n",
      "epoch 489, loss 45.834948\n",
      "epoch 490, loss 45.809833\n",
      "epoch 491, loss 45.784835\n",
      "epoch 492, loss 45.760014\n",
      "epoch 493, loss 45.735339\n",
      "epoch 494, loss 45.710797\n",
      "epoch 495, loss 45.686383\n",
      "epoch 496, loss 45.662114\n",
      "epoch 497, loss 45.638013\n",
      "epoch 498, loss 45.614029\n",
      "epoch 499, loss 45.590190\n",
      "epoch 500, loss 45.566503\n",
      "epoch 501, loss 45.542933\n",
      "epoch 502, loss 45.519513\n",
      "epoch 503, loss 45.496233\n",
      "epoch 504, loss 45.473077\n",
      "epoch 505, loss 45.450065\n",
      "epoch 506, loss 45.427187\n",
      "epoch 507, loss 45.404436\n",
      "epoch 508, loss 45.381811\n",
      "epoch 509, loss 45.359330\n",
      "epoch 510, loss 45.336978\n",
      "epoch 511, loss 45.314739\n",
      "epoch 512, loss 45.292637\n",
      "epoch 513, loss 45.270669\n",
      "epoch 514, loss 45.248826\n",
      "epoch 515, loss 45.227104\n",
      "epoch 516, loss 45.205492\n",
      "epoch 517, loss 45.184027\n",
      "epoch 518, loss 45.162687\n",
      "epoch 519, loss 45.141470\n",
      "epoch 520, loss 45.120364\n",
      "epoch 521, loss 45.099378\n",
      "epoch 522, loss 45.078516\n",
      "epoch 523, loss 45.057779\n",
      "epoch 524, loss 45.037166\n",
      "epoch 525, loss 45.016647\n",
      "epoch 526, loss 44.996272\n",
      "epoch 527, loss 44.975996\n",
      "epoch 528, loss 44.955848\n",
      "epoch 529, loss 44.935800\n",
      "epoch 530, loss 44.915892\n",
      "epoch 531, loss 44.896078\n",
      "epoch 532, loss 44.876386\n",
      "epoch 533, loss 44.856789\n",
      "epoch 534, loss 44.837322\n",
      "epoch 535, loss 44.817965\n",
      "epoch 536, loss 44.798718\n",
      "epoch 537, loss 44.779577\n",
      "epoch 538, loss 44.760550\n",
      "epoch 539, loss 44.741620\n",
      "epoch 540, loss 44.722807\n",
      "epoch 541, loss 44.704115\n",
      "epoch 542, loss 44.685505\n",
      "epoch 543, loss 44.667002\n",
      "epoch 544, loss 44.648608\n",
      "epoch 545, loss 44.630314\n",
      "epoch 546, loss 44.612149\n",
      "epoch 547, loss 44.594057\n",
      "epoch 548, loss 44.576082\n",
      "epoch 549, loss 44.558202\n",
      "epoch 550, loss 44.540434\n",
      "epoch 551, loss 44.522753\n",
      "epoch 552, loss 44.505176\n",
      "epoch 553, loss 44.487686\n",
      "epoch 554, loss 44.470316\n",
      "epoch 555, loss 44.453033\n",
      "epoch 556, loss 44.435850\n",
      "epoch 557, loss 44.418766\n",
      "epoch 558, loss 44.401790\n",
      "epoch 559, loss 44.384901\n",
      "epoch 560, loss 44.368106\n",
      "epoch 561, loss 44.351393\n",
      "epoch 562, loss 44.334785\n",
      "epoch 563, loss 44.318262\n",
      "epoch 564, loss 44.301840\n",
      "epoch 565, loss 44.285514\n",
      "epoch 566, loss 44.269264\n",
      "epoch 567, loss 44.253110\n",
      "epoch 568, loss 44.237048\n",
      "epoch 569, loss 44.221079\n",
      "epoch 570, loss 44.205191\n",
      "epoch 571, loss 44.189407\n",
      "epoch 572, loss 44.173703\n",
      "epoch 573, loss 44.158087\n",
      "epoch 574, loss 44.142566\n",
      "epoch 575, loss 44.127115\n",
      "epoch 576, loss 44.111750\n",
      "epoch 577, loss 44.096487\n",
      "epoch 578, loss 44.081290\n",
      "epoch 579, loss 44.066197\n",
      "epoch 580, loss 44.051180\n",
      "epoch 581, loss 44.036240\n",
      "epoch 582, loss 44.021384\n",
      "epoch 583, loss 44.006601\n",
      "epoch 584, loss 43.991924\n",
      "epoch 585, loss 43.977330\n",
      "epoch 586, loss 43.962801\n",
      "epoch 587, loss 43.948346\n",
      "epoch 588, loss 43.933987\n",
      "epoch 589, loss 43.919689\n",
      "epoch 590, loss 43.905471\n",
      "epoch 591, loss 43.891349\n",
      "epoch 592, loss 43.877294\n",
      "epoch 593, loss 43.863321\n",
      "epoch 594, loss 43.849420\n",
      "epoch 595, loss 43.835600\n",
      "epoch 596, loss 43.821852\n",
      "epoch 597, loss 43.808161\n",
      "epoch 598, loss 43.794563\n",
      "epoch 599, loss 43.781040\n",
      "epoch 600, loss 43.767604\n",
      "epoch 601, loss 43.754240\n",
      "epoch 602, loss 43.740939\n",
      "epoch 603, loss 43.727704\n",
      "epoch 604, loss 43.714546\n",
      "epoch 605, loss 43.701466\n",
      "epoch 606, loss 43.688460\n",
      "epoch 607, loss 43.675507\n",
      "epoch 608, loss 43.662650\n",
      "epoch 609, loss 43.649848\n",
      "epoch 610, loss 43.637124\n",
      "epoch 611, loss 43.624475\n",
      "epoch 612, loss 43.611888\n",
      "epoch 613, loss 43.599371\n",
      "epoch 614, loss 43.586909\n",
      "epoch 615, loss 43.574526\n",
      "epoch 616, loss 43.562212\n",
      "epoch 617, loss 43.549965\n",
      "epoch 618, loss 43.537766\n",
      "epoch 619, loss 43.525638\n",
      "epoch 620, loss 43.513566\n",
      "epoch 621, loss 43.501572\n",
      "epoch 622, loss 43.489645\n",
      "epoch 623, loss 43.477788\n",
      "epoch 624, loss 43.465992\n",
      "epoch 625, loss 43.454259\n",
      "epoch 626, loss 43.442585\n",
      "epoch 627, loss 43.430972\n",
      "epoch 628, loss 43.419432\n",
      "epoch 629, loss 43.407942\n",
      "epoch 630, loss 43.396545\n",
      "epoch 631, loss 43.385159\n",
      "epoch 632, loss 43.373861\n",
      "epoch 633, loss 43.362629\n",
      "epoch 634, loss 43.351446\n",
      "epoch 635, loss 43.340325\n",
      "epoch 636, loss 43.329255\n",
      "epoch 637, loss 43.318249\n",
      "epoch 638, loss 43.307303\n",
      "epoch 639, loss 43.296413\n",
      "epoch 640, loss 43.285587\n",
      "epoch 641, loss 43.274813\n",
      "epoch 642, loss 43.264114\n",
      "epoch 643, loss 43.253449\n",
      "epoch 644, loss 43.242851\n",
      "epoch 645, loss 43.232306\n",
      "epoch 646, loss 43.221820\n",
      "epoch 647, loss 43.211380\n",
      "epoch 648, loss 43.201006\n",
      "epoch 649, loss 43.190698\n",
      "epoch 650, loss 43.180427\n",
      "epoch 651, loss 43.170215\n",
      "epoch 652, loss 43.160056\n",
      "epoch 653, loss 43.149957\n",
      "epoch 654, loss 43.139905\n",
      "epoch 655, loss 43.129897\n",
      "epoch 656, loss 43.119949\n",
      "epoch 657, loss 43.110058\n",
      "epoch 658, loss 43.100225\n",
      "epoch 659, loss 43.090427\n",
      "epoch 660, loss 43.080673\n",
      "epoch 661, loss 43.070971\n",
      "epoch 662, loss 43.061330\n",
      "epoch 663, loss 43.051741\n",
      "epoch 664, loss 43.042188\n",
      "epoch 665, loss 43.032709\n",
      "epoch 666, loss 43.023270\n",
      "epoch 667, loss 43.013873\n",
      "epoch 668, loss 43.004536\n",
      "epoch 669, loss 42.995253\n",
      "epoch 670, loss 42.986011\n",
      "epoch 671, loss 42.976807\n",
      "epoch 672, loss 42.967646\n",
      "epoch 673, loss 42.958551\n",
      "epoch 674, loss 42.949492\n",
      "epoch 675, loss 42.940467\n",
      "epoch 676, loss 42.931497\n",
      "epoch 677, loss 42.922585\n",
      "epoch 678, loss 42.913727\n",
      "epoch 679, loss 42.904887\n",
      "epoch 680, loss 42.896125\n",
      "epoch 681, loss 42.887379\n",
      "epoch 682, loss 42.878687\n",
      "epoch 683, loss 42.870053\n",
      "epoch 684, loss 42.861443\n",
      "epoch 685, loss 42.852884\n",
      "epoch 686, loss 42.844380\n",
      "epoch 687, loss 42.835898\n",
      "epoch 688, loss 42.827473\n",
      "epoch 689, loss 42.819078\n",
      "epoch 690, loss 42.810751\n",
      "epoch 691, loss 42.802444\n",
      "epoch 692, loss 42.794188\n",
      "epoch 693, loss 42.785974\n",
      "epoch 694, loss 42.777790\n",
      "epoch 695, loss 42.769662\n",
      "epoch 696, loss 42.761564\n",
      "epoch 697, loss 42.753520\n",
      "epoch 698, loss 42.745519\n",
      "epoch 699, loss 42.737551\n",
      "epoch 700, loss 42.729613\n",
      "epoch 701, loss 42.721737\n",
      "epoch 702, loss 42.713870\n",
      "epoch 703, loss 42.706064\n",
      "epoch 704, loss 42.698289\n",
      "epoch 705, loss 42.690553\n",
      "epoch 706, loss 42.682847\n",
      "epoch 707, loss 42.675186\n",
      "epoch 708, loss 42.667574\n",
      "epoch 709, loss 42.659996\n",
      "epoch 710, loss 42.652456\n",
      "epoch 711, loss 42.644953\n",
      "epoch 712, loss 42.637492\n",
      "epoch 713, loss 42.630051\n",
      "epoch 714, loss 42.622654\n",
      "epoch 715, loss 42.615296\n",
      "epoch 716, loss 42.607979\n",
      "epoch 717, loss 42.600686\n",
      "epoch 718, loss 42.593442\n",
      "epoch 719, loss 42.586226\n",
      "epoch 720, loss 42.579052\n",
      "epoch 721, loss 42.571912\n",
      "epoch 722, loss 42.564808\n",
      "epoch 723, loss 42.557743\n",
      "epoch 724, loss 42.550696\n",
      "epoch 725, loss 42.543666\n",
      "epoch 726, loss 42.536703\n",
      "epoch 727, loss 42.529763\n",
      "epoch 728, loss 42.522857\n",
      "epoch 729, loss 42.515977\n",
      "epoch 730, loss 42.509147\n",
      "epoch 731, loss 42.502334\n",
      "epoch 732, loss 42.495546\n",
      "epoch 733, loss 42.488805\n",
      "epoch 734, loss 42.482090\n",
      "epoch 735, loss 42.475420\n",
      "epoch 736, loss 42.468770\n",
      "epoch 737, loss 42.462163\n",
      "epoch 738, loss 42.455573\n",
      "epoch 739, loss 42.449035\n",
      "epoch 740, loss 42.442512\n",
      "epoch 741, loss 42.436033\n",
      "epoch 742, loss 42.429578\n",
      "epoch 743, loss 42.423147\n",
      "epoch 744, loss 42.416764\n",
      "epoch 745, loss 42.410410\n",
      "epoch 746, loss 42.404077\n",
      "epoch 747, loss 42.397794\n",
      "epoch 748, loss 42.391511\n",
      "epoch 749, loss 42.385282\n",
      "epoch 750, loss 42.379080\n",
      "epoch 751, loss 42.372893\n",
      "epoch 752, loss 42.366746\n",
      "epoch 753, loss 42.360625\n",
      "epoch 754, loss 42.354534\n",
      "epoch 755, loss 42.348472\n",
      "epoch 756, loss 42.342451\n",
      "epoch 757, loss 42.336435\n",
      "epoch 758, loss 42.330447\n",
      "epoch 759, loss 42.324497\n",
      "epoch 760, loss 42.318558\n",
      "epoch 761, loss 42.312661\n",
      "epoch 762, loss 42.306793\n",
      "epoch 763, loss 42.300956\n",
      "epoch 764, loss 42.295129\n",
      "epoch 765, loss 42.289336\n",
      "epoch 766, loss 42.283563\n",
      "epoch 767, loss 42.277811\n",
      "epoch 768, loss 42.272110\n",
      "epoch 769, loss 42.266411\n",
      "epoch 770, loss 42.260758\n",
      "epoch 771, loss 42.255132\n",
      "epoch 772, loss 42.249516\n",
      "epoch 773, loss 42.243940\n",
      "epoch 774, loss 42.238382\n",
      "epoch 775, loss 42.232839\n",
      "epoch 776, loss 42.227341\n",
      "epoch 777, loss 42.221851\n",
      "epoch 778, loss 42.216398\n",
      "epoch 779, loss 42.210952\n",
      "epoch 780, loss 42.205552\n",
      "epoch 781, loss 42.200151\n",
      "epoch 782, loss 42.194791\n",
      "epoch 783, loss 42.189448\n",
      "epoch 784, loss 42.184132\n",
      "epoch 785, loss 42.178847\n",
      "epoch 786, loss 42.173580\n",
      "epoch 787, loss 42.168337\n",
      "epoch 788, loss 42.163100\n",
      "epoch 789, loss 42.157910\n",
      "epoch 790, loss 42.152730\n",
      "epoch 791, loss 42.147585\n",
      "epoch 792, loss 42.142450\n",
      "epoch 793, loss 42.137344\n",
      "epoch 794, loss 42.132249\n",
      "epoch 795, loss 42.127204\n",
      "epoch 796, loss 42.122171\n",
      "epoch 797, loss 42.117162\n",
      "epoch 798, loss 42.112177\n",
      "epoch 799, loss 42.107201\n",
      "epoch 800, loss 42.102253\n",
      "epoch 801, loss 42.097310\n",
      "epoch 802, loss 42.092393\n",
      "epoch 803, loss 42.087492\n",
      "epoch 804, loss 42.082620\n",
      "epoch 805, loss 42.077765\n",
      "epoch 806, loss 42.072940\n",
      "epoch 807, loss 42.068128\n",
      "epoch 808, loss 42.063353\n",
      "epoch 809, loss 42.058584\n",
      "epoch 810, loss 42.053831\n",
      "epoch 811, loss 42.049103\n",
      "epoch 812, loss 42.044390\n",
      "epoch 813, loss 42.039711\n",
      "epoch 814, loss 42.035050\n",
      "epoch 815, loss 42.030401\n",
      "epoch 816, loss 42.025774\n",
      "epoch 817, loss 42.021164\n",
      "epoch 818, loss 42.016580\n",
      "epoch 819, loss 42.011992\n",
      "epoch 820, loss 42.007444\n",
      "epoch 821, loss 42.002908\n",
      "epoch 822, loss 41.998401\n",
      "epoch 823, loss 41.993907\n",
      "epoch 824, loss 41.989432\n",
      "epoch 825, loss 41.984969\n",
      "epoch 826, loss 41.980543\n",
      "epoch 827, loss 41.976130\n",
      "epoch 828, loss 41.971726\n",
      "epoch 829, loss 41.967349\n",
      "epoch 830, loss 41.962990\n",
      "epoch 831, loss 41.958634\n",
      "epoch 832, loss 41.954297\n",
      "epoch 833, loss 41.949987\n",
      "epoch 834, loss 41.945691\n",
      "epoch 835, loss 41.941432\n",
      "epoch 836, loss 41.937161\n",
      "epoch 837, loss 41.932916\n",
      "epoch 838, loss 41.928674\n",
      "epoch 839, loss 41.924457\n",
      "epoch 840, loss 41.920265\n",
      "epoch 841, loss 41.916085\n",
      "epoch 842, loss 41.911924\n",
      "epoch 843, loss 41.907790\n",
      "epoch 844, loss 41.903662\n",
      "epoch 845, loss 41.899559\n",
      "epoch 846, loss 41.895473\n",
      "epoch 847, loss 41.891397\n",
      "epoch 848, loss 41.887335\n",
      "epoch 849, loss 41.883296\n",
      "epoch 850, loss 41.879250\n",
      "epoch 851, loss 41.875249\n",
      "epoch 852, loss 41.871242\n",
      "epoch 853, loss 41.867260\n",
      "epoch 854, loss 41.863285\n",
      "epoch 855, loss 41.859335\n",
      "epoch 856, loss 41.855393\n",
      "epoch 857, loss 41.851487\n",
      "epoch 858, loss 41.847571\n",
      "epoch 859, loss 41.843678\n",
      "epoch 860, loss 41.839799\n",
      "epoch 861, loss 41.835938\n",
      "epoch 862, loss 41.832086\n",
      "epoch 863, loss 41.828260\n",
      "epoch 864, loss 41.824439\n",
      "epoch 865, loss 41.820633\n",
      "epoch 866, loss 41.816859\n",
      "epoch 867, loss 41.813089\n",
      "epoch 868, loss 41.809337\n",
      "epoch 869, loss 41.805578\n",
      "epoch 870, loss 41.801843\n",
      "epoch 871, loss 41.798119\n",
      "epoch 872, loss 41.794426\n",
      "epoch 873, loss 41.790719\n",
      "epoch 874, loss 41.787040\n",
      "epoch 875, loss 41.783378\n",
      "epoch 876, loss 41.779709\n",
      "epoch 877, loss 41.776067\n",
      "epoch 878, loss 41.772463\n",
      "epoch 879, loss 41.768841\n",
      "epoch 880, loss 41.765246\n",
      "epoch 881, loss 41.761658\n",
      "epoch 882, loss 41.758089\n",
      "epoch 883, loss 41.754529\n",
      "epoch 884, loss 41.750988\n",
      "epoch 885, loss 41.747434\n",
      "epoch 886, loss 41.743921\n",
      "epoch 887, loss 41.740403\n",
      "epoch 888, loss 41.736885\n",
      "epoch 889, loss 41.733392\n",
      "epoch 890, loss 41.729918\n",
      "epoch 891, loss 41.726452\n",
      "epoch 892, loss 41.722999\n",
      "epoch 893, loss 41.719562\n",
      "epoch 894, loss 41.716145\n",
      "epoch 895, loss 41.712713\n",
      "epoch 896, loss 41.709311\n",
      "epoch 897, loss 41.705927\n",
      "epoch 898, loss 41.702550\n",
      "epoch 899, loss 41.699181\n",
      "epoch 900, loss 41.695821\n",
      "epoch 901, loss 41.692479\n",
      "epoch 902, loss 41.689144\n",
      "epoch 903, loss 41.685821\n",
      "epoch 904, loss 41.682506\n",
      "epoch 905, loss 41.679198\n",
      "epoch 906, loss 41.675904\n",
      "epoch 907, loss 41.672625\n",
      "epoch 908, loss 41.669352\n",
      "epoch 909, loss 41.666100\n",
      "epoch 910, loss 41.662858\n",
      "epoch 911, loss 41.659622\n",
      "epoch 912, loss 41.656393\n",
      "epoch 913, loss 41.653183\n",
      "epoch 914, loss 41.649983\n",
      "epoch 915, loss 41.646795\n",
      "epoch 916, loss 41.643617\n",
      "epoch 917, loss 41.640451\n",
      "epoch 918, loss 41.637290\n",
      "epoch 919, loss 41.634140\n",
      "epoch 920, loss 41.630985\n",
      "epoch 921, loss 41.627839\n",
      "epoch 922, loss 41.624728\n",
      "epoch 923, loss 41.621610\n",
      "epoch 924, loss 41.618516\n",
      "epoch 925, loss 41.615427\n",
      "epoch 926, loss 41.612348\n",
      "epoch 927, loss 41.609259\n",
      "epoch 928, loss 41.606187\n",
      "epoch 929, loss 41.603142\n",
      "epoch 930, loss 41.600084\n",
      "epoch 931, loss 41.597040\n",
      "epoch 932, loss 41.594029\n",
      "epoch 933, loss 41.591004\n",
      "epoch 934, loss 41.588006\n",
      "epoch 935, loss 41.584992\n",
      "epoch 936, loss 41.582005\n",
      "epoch 937, loss 41.579026\n",
      "epoch 938, loss 41.576054\n",
      "epoch 939, loss 41.573095\n",
      "epoch 940, loss 41.570139\n",
      "epoch 941, loss 41.567198\n",
      "epoch 942, loss 41.564254\n",
      "epoch 943, loss 41.561317\n",
      "epoch 944, loss 41.558392\n",
      "epoch 945, loss 41.555472\n",
      "epoch 946, loss 41.552570\n",
      "epoch 947, loss 41.549673\n",
      "epoch 948, loss 41.546790\n",
      "epoch 949, loss 41.543915\n",
      "epoch 950, loss 41.541043\n",
      "epoch 951, loss 41.538189\n",
      "epoch 952, loss 41.535352\n",
      "epoch 953, loss 41.532513\n",
      "epoch 954, loss 41.529671\n",
      "epoch 955, loss 41.526847\n",
      "epoch 956, loss 41.524022\n",
      "epoch 957, loss 41.521202\n",
      "epoch 958, loss 41.518399\n",
      "epoch 959, loss 41.515600\n",
      "epoch 960, loss 41.512810\n",
      "epoch 961, loss 41.510023\n",
      "epoch 962, loss 41.507255\n",
      "epoch 963, loss 41.504483\n",
      "epoch 964, loss 41.501722\n",
      "epoch 965, loss 41.498971\n",
      "epoch 966, loss 41.496228\n",
      "epoch 967, loss 41.493493\n",
      "epoch 968, loss 41.490764\n",
      "epoch 969, loss 41.488043\n",
      "epoch 970, loss 41.485327\n",
      "epoch 971, loss 41.482610\n",
      "epoch 972, loss 41.479901\n",
      "epoch 973, loss 41.477216\n",
      "epoch 974, loss 41.474529\n",
      "epoch 975, loss 41.471853\n",
      "epoch 976, loss 41.469192\n",
      "epoch 977, loss 41.466518\n",
      "epoch 978, loss 41.463882\n",
      "epoch 979, loss 41.461235\n",
      "epoch 980, loss 41.458593\n",
      "epoch 981, loss 41.455963\n",
      "epoch 982, loss 41.453340\n",
      "epoch 983, loss 41.450716\n",
      "epoch 984, loss 41.448103\n",
      "epoch 985, loss 41.445485\n",
      "epoch 986, loss 41.442868\n",
      "epoch 987, loss 41.440250\n",
      "epoch 988, loss 41.437646\n",
      "epoch 989, loss 41.435063\n",
      "epoch 990, loss 41.432491\n",
      "epoch 991, loss 41.429914\n",
      "epoch 992, loss 41.427351\n",
      "epoch 993, loss 41.424790\n",
      "epoch 994, loss 41.422253\n",
      "epoch 995, loss 41.419718\n",
      "epoch 996, loss 41.417173\n",
      "epoch 997, loss 41.414642\n",
      "epoch 998, loss 41.412114\n",
      "epoch 999, loss 41.409574\n",
      "epoch 1000, loss 41.407063\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for X, y in data_iter(batch_size, X_train, y_train):   \n",
    "        \n",
    "        \n",
    "        l = squared_loss(linreg(X,w,b), y)\n",
    "        l.backward()\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j in range(len(w)):\n",
    "                w[j] -= lr * w.grad[j]\n",
    "            b -= lr * b.grad\n",
    "            \n",
    "            # Manually zero the gradients after updating weights\n",
    "            w.grad=None\n",
    "            b.grad=None\n",
    "\n",
    "    train_l = squared_loss(linreg(X_train, w, b), y_train)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0747,  0.0765,  0.0344, -1.1369,  0.8105,  2.2408,  0.0711,  0.2236,\n",
       "          0.0512, -0.0084,  0.2466,  0.0174, -0.7071], requires_grad=True),\n",
       " tensor([0.0039], requires_grad=True))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.9368, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_loss(linreg(X_test,w,b), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
